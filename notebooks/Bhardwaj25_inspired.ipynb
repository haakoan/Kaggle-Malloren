{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14436762,"sourceType":"datasetVersion","datasetId":9221482},{"sourceId":14610868,"sourceType":"datasetVersion","datasetId":9332666},{"sourceId":14611013,"sourceType":"datasetVersion","datasetId":9284963,"isSourceIdPinned":true},{"sourceId":14637655,"sourceType":"datasetVersion","datasetId":9350625}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Bhardwaj et al. 2025 TDE Classifier - Adapted for MALLORN\n#With GRU predictions added as feature\n\n\n!pip install george -q\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport george\nfrom george import kernels\nfrom scipy.optimize import minimize\n\nimport torch\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n\nSEED = 42\nnp.random.seed(SEED)\n\nprint(f\"george version: {george.__version__}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:37:45.810207Z","iopub.execute_input":"2026-01-31T08:37:45.810561Z","iopub.status.idle":"2026-01-31T08:37:59.024464Z","shell.execute_reply.started":"2026-01-31T08:37:45.810531Z","shell.execute_reply":"2026-01-31T08:37:59.023608Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.6/395.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hgeorge version: 0.4.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/mallorn-astronomical-classification-challenge\")\n\ntrain_log = pd.read_csv(DATA_DIR / \"train_log.csv\")\ntest_log = pd.read_csv(DATA_DIR / \"test_log.csv\")\n\nprint(f\"Train: {len(train_log)} objects, {train_log['target'].sum()} TDEs ({100*train_log['target'].mean():.1f}%)\")\nprint(f\"Test: {len(test_log)} objects\")\n\n# Load light curves\nsplit_dirs = sorted([p for p in DATA_DIR.glob(\"split_*\") if p.is_dir()])\nprint(f\"Found {len(split_dirs)} splits\")\n\ntrain_lc = pd.concat([pd.read_csv(d / \"train_full_lightcurves.csv\") for d in split_dirs], ignore_index=True)\ntest_lc = pd.concat([pd.read_csv(d / \"test_full_lightcurves.csv\") for d in split_dirs], ignore_index=True)\n\nprint(f\"Train LC: {len(train_lc):,} rows\")\nprint(f\"Test LC: {len(test_lc):,} rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:37:59.026201Z","iopub.execute_input":"2026-01-31T08:37:59.026581Z","iopub.status.idle":"2026-01-31T08:38:02.231595Z","shell.execute_reply.started":"2026-01-31T08:37:59.026554Z","shell.execute_reply":"2026-01-31T08:38:02.230789Z"}},"outputs":[{"name":"stdout","text":"Train: 3043 objects, 148 TDEs (4.9%)\nTest: 7135 objects\nFound 20 splits\nTrain LC: 479,384 rows\nTest LC: 1,145,125 rows\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"FILTER_WAVELENGTHS = {\n    'u': 3670.69, 'g': 4826.85, 'r': 6223.24,\n    'i': 7545.98, 'z': 8590.90, 'y': 9710.28\n}\n\ndef compute_gp(sub, obj_id):\n    \"\"\"Fit 2D GP (time × wavelength) using george library.\"\"\"\n    try:\n        t = np.array(sub['Time (MJD)'], dtype=float)\n        flux = np.array(sub['Flux'], dtype=float)\n        fluxerr = np.array(sub['Flux_err'], dtype=float)\n        band = np.array([FILTER_WAVELENGTHS.get(b.strip().lower(), np.nan) for b in sub['Filter']], dtype=float)\n        \n        x = np.vstack([t, band]).T\n        mask = np.isfinite(flux) & np.isfinite(fluxerr) & np.all(np.isfinite(x), axis=1) & (flux > 0)\n        x, flux, fluxerr = x[mask], flux[mask], fluxerr[mask]\n        \n        if len(flux) < 5:\n            return None, None, None, None\n        \n        snr = np.abs(flux) / np.sqrt(fluxerr**2 + (0.01 * np.max(flux))**2)\n        scale = np.abs(flux[np.argmax(snr)])\n        \n        kernel = (0.5 * scale)**2 * george.kernels.Matern32Kernel([100**2, 6000**2], ndim=2)\n        gp = george.GP(kernel, solver=george.HODLRSolver)\n        gp.compute(x, fluxerr)\n        \n        def neg_ln_like(p):\n            gp.set_parameter_vector(p)\n            return -gp.log_likelihood(flux)\n        \n        def grad_neg_ln_like(p):\n            gp.set_parameter_vector(p)\n            return -gp.grad_log_likelihood(flux)\n        \n        p0 = gp.get_parameter_vector()\n        for _ in range(3):\n            result = minimize(neg_ln_like, p0, jac=grad_neg_ln_like, method='L-BFGS-B')\n            if result.success:\n                gp.set_parameter_vector(result.x)\n                break\n            p0 = p0 + np.random.normal(0, 0.1, size=p0.shape)\n        \n        return gp, flux, x, gp.get_parameter_vector()\n    except:\n        return None, None, None, None\n\n\ndef find_peak_and_times(gp, sub, flux, band='g'):\n    \"\"\"Find peak and rise/fade times for one band.\"\"\"\n    t_min, t_max = sub['Time (MJD)'].min(), sub['Time (MJD)'].max()\n    mjd = np.linspace(t_min - 50, t_max + 75, 500)\n    wl = FILTER_WAVELENGTHS[band]\n    x_pred = np.vstack([mjd, wl * np.ones_like(mjd)]).T\n    \n    try:\n        mean_pred, _ = gp.predict(flux, x_pred, return_var=True)\n    except:\n        return None, None, None, None\n    \n    peak_idx = np.argmax(mean_pred)\n    peak_mjd = mjd[peak_idx]\n    peak_flux = mean_pred[peak_idx]\n    \n    if peak_flux <= 0:\n        return peak_mjd, peak_flux, None, None\n    \n    thr = peak_flux / 2.512\n    \n    pre = mean_pred[:peak_idx]\n    rise_idx = np.where(pre <= thr)[0]\n    rise_time = peak_mjd - mjd[rise_idx[-1]] if len(rise_idx) > 0 else None\n    \n    post = mean_pred[peak_idx:]\n    fade_idx = np.where(post <= thr)[0]\n    fade_time = mjd[peak_idx + fade_idx[0]] - peak_mjd if len(fade_idx) > 0 else None\n    \n    return peak_mjd, peak_flux, rise_time, fade_time\n\n\ndef compute_colors(gp, sub, flux, peak_mjd, bands=['g', 'r', 'i']):\n    \"\"\"Compute mean colors pre/post peak.\"\"\"\n    colors = {}\n    \n    for b in bands:\n        wl = FILTER_WAVELENGTHS[b]\n        \n        # Pre-peak: -30 to 0 days\n        t_pre = np.linspace(peak_mjd - 30, peak_mjd, 50)\n        x_pre = np.vstack([t_pre, wl * np.ones_like(t_pre)]).T\n        \n        # Post-peak: 0 to +30 days  \n        t_post = np.linspace(peak_mjd, peak_mjd + 30, 50)\n        x_post = np.vstack([t_post, wl * np.ones_like(t_post)]).T\n        \n        try:\n            flux_pre, _ = gp.predict(flux, x_pre, return_var=True)\n            flux_post, _ = gp.predict(flux, x_post, return_var=True)\n            colors[f'{b}_pre'] = np.mean(flux_pre[flux_pre > 0]) if np.any(flux_pre > 0) else np.nan\n            colors[f'{b}_post'] = np.mean(flux_post[flux_post > 0]) if np.any(flux_post > 0) else np.nan\n        except:\n            colors[f'{b}_pre'] = np.nan\n            colors[f'{b}_post'] = np.nan\n    \n    return colors\n\n\ndef flux_to_mag(flux):\n    \"\"\"Convert flux to magnitude.\"\"\"\n    if flux is None or flux <= 0 or np.isnan(flux):\n        return np.nan\n    return -2.5 * np.log10(flux)\n\n\ndef extract_bhardwaj_features(obj_id, lc_df):\n    \"\"\"Extract all 13 Bhardwaj features for one object.\"\"\"\n    sub = lc_df[lc_df['object_id'] == obj_id]\n    \n    if len(sub) < 10:\n        return None\n    \n    gp, flux, x, params = compute_gp(sub, obj_id)\n    \n    if gp is None:\n        return None\n    \n    # GP hyperparameters (3 features)\n    features = {\n        'Amplitude': np.exp(params[0]),\n        'LengthScale_Time': np.sqrt(np.exp(params[1])),\n        'LengthScale_Wavelength': np.sqrt(np.exp(params[2])) if len(params) > 2 else np.nan,\n    }\n    \n    # Timing features (2 features)\n    peak_mjd, peak_flux, rise_time, fade_time = find_peak_and_times(gp, sub, flux, 'g')\n    features['Rise_Time'] = rise_time\n    features['Fade_Time'] = fade_time\n    \n    if peak_mjd is None:\n        return features\n    \n    # Color features (8 features)\n    colors = compute_colors(gp, sub, flux, peak_mjd, ['g', 'r', 'i'])\n    \n    g_pre, r_pre, i_pre = colors.get('g_pre'), colors.get('r_pre'), colors.get('i_pre')\n    g_post, r_post, i_post = colors.get('g_post'), colors.get('r_post'), colors.get('i_post')\n    \n    # Mean colors (4 features)\n    features['Mean_Color_Pre_gr'] = flux_to_mag(g_pre) - flux_to_mag(r_pre) if g_pre and r_pre else np.nan\n    features['Mean_Color_Post_gr'] = flux_to_mag(g_post) - flux_to_mag(r_post) if g_post and r_post else np.nan\n    features['Mean_Color_Pre_ri'] = flux_to_mag(r_pre) - flux_to_mag(i_pre) if r_pre and i_pre else np.nan\n    features['Mean_Color_Post_ri'] = flux_to_mag(r_post) - flux_to_mag(i_post) if r_post and i_post else np.nan\n    \n    # Color slopes (4 features)\n    features['Slope_Pre_gr'] = (features['Mean_Color_Post_gr'] - features['Mean_Color_Pre_gr']) / 30 if not np.isnan(features.get('Mean_Color_Pre_gr', np.nan)) else np.nan\n    features['Slope_Post_gr'] = features['Slope_Pre_gr']  # Simplified\n    features['Slope_Pre_ri'] = (features['Mean_Color_Post_ri'] - features['Mean_Color_Pre_ri']) / 30 if not np.isnan(features.get('Mean_Color_Pre_ri', np.nan)) else np.nan\n    features['Slope_Post_ri'] = features['Slope_Pre_ri']  # Simplified\n    \n    return features\n\n\n# Feature column names\nBHARDWAJ_FEATURES = [\n    'Amplitude', 'LengthScale_Time', 'LengthScale_Wavelength',\n    'Rise_Time', 'Fade_Time',\n    'Mean_Color_Pre_gr', 'Mean_Color_Post_gr', 'Mean_Color_Pre_ri', 'Mean_Color_Post_ri',\n    'Slope_Pre_gr', 'Slope_Post_gr', 'Slope_Pre_ri', 'Slope_Post_ri'\n]\n\nprint(f\"Defined {len(BHARDWAJ_FEATURES)} Bhardwaj features\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:38:02.232708Z","iopub.execute_input":"2026-01-31T08:38:02.233361Z","iopub.status.idle":"2026-01-31T08:38:02.259498Z","shell.execute_reply.started":"2026-01-31T08:38:02.233333Z","shell.execute_reply":"2026-01-31T08:38:02.258382Z"}},"outputs":[{"name":"stdout","text":"Defined 13 Bhardwaj features\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\ntrain_features = []\nfor obj_id in tqdm(train_log['object_id']):\n    feats = extract_bhardwaj_features(obj_id, train_lc)\n    if feats:\n        feats['object_id'] = obj_id\n        train_features.append(feats)\n    else:\n        train_features.append({'object_id': obj_id})\n\ntrain_feats = pd.DataFrame(train_features)\nprint(f\"Train features: {train_feats.shape}\")\n\n\ntest_features = []\nfor obj_id in tqdm(test_log['object_id']):\n    feats = extract_bhardwaj_features(obj_id, test_lc)\n    if feats:\n        feats['object_id'] = obj_id\n        test_features.append(feats)\n    else:\n        test_features.append({'object_id': obj_id})\n\ntest_feats = pd.DataFrame(test_features)\nprint(f\"Test features: {test_feats.shape}\")\n\n# Check coverage\nfor col in BHARDWAJ_FEATURES:\n    if col in train_feats.columns:\n        cov = train_feats[col].notna().mean() * 100\n        print(f\"  {col}: {cov:.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T08:38:02.261458Z","iopub.execute_input":"2026-01-31T08:38:02.261943Z","iopub.status.idle":"2026-01-31T09:17:41.051442Z","shell.execute_reply.started":"2026-01-31T08:38:02.261883Z","shell.execute_reply":"2026-01-31T09:17:41.050642Z"}},"outputs":[{"name":"stdout","text":"Extracting Bhardwaj features for training set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3043 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9633894373dd49278b454605270b221e"}},"metadata":{}},{"name":"stdout","text":"Train features: (3043, 14)\n\nExtracting Bhardwaj features for test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7135 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51aaa6b1aeea4426aaff101e9d96e6cf"}},"metadata":{}},{"name":"stdout","text":"Test features: (7135, 14)\n\nFeature coverage (train):\n  Amplitude: 100.0%\n  LengthScale_Time: 100.0%\n  LengthScale_Wavelength: 100.0%\n  Rise_Time: 97.1%\n  Fade_Time: 80.2%\n  Mean_Color_Pre_gr: 100.0%\n  Mean_Color_Post_gr: 100.0%\n  Mean_Color_Pre_ri: 99.9%\n  Mean_Color_Post_ri: 99.9%\n  Slope_Pre_gr: 100.0%\n  Slope_Post_gr: 100.0%\n  Slope_Pre_ri: 99.9%\n  Slope_Post_ri: 99.9%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nGRU_CACHE_PATH = Path(\"/kaggle/input/mallorenlargemodels/gru_cache.pt\") \n\nprint(f\"Loading GRU from: {GRU_CACHE_PATH}\")\n\ngru_cache = torch.load(GRU_CACHE_PATH, map_location='cpu', weights_only=False)\ngru_oof_raw = gru_cache['oof']\ngru_test_raw = gru_cache['test']\n    \nprint(f\"  Loaded from: {gru_cache.get('timestamp', 'unknown')}\")\nprint(f\"  GRU OOF shape: {gru_oof_raw.shape}\")\nprint(f\"  GRU test shape: {gru_test_raw.shape}\")\n    \n# VALIDATE: Check GRU is working\ny_for_validation = train_log['target'].values\nauc_raw = roc_auc_score(y_for_validation, gru_oof_raw)\nprint(f\"\\n  RAW GRU AUC: {auc_raw:.4f}\")\nprint(f\"  RAW GRU - TDE mean: {gru_oof_raw[y_for_validation==1].mean():.4f}\")\nprint(f\"  RAW GRU - Non-TDE mean: {gru_oof_raw[y_for_validation==0].mean():.4f}\")\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:34:35.147726Z","iopub.execute_input":"2026-01-31T09:34:35.148652Z","iopub.status.idle":"2026-01-31T09:34:35.176067Z","shell.execute_reply.started":"2026-01-31T09:34:35.148620Z","shell.execute_reply":"2026-01-31T09:34:35.174805Z"}},"outputs":[{"name":"stdout","text":"Loading GRU from: /kaggle/input/mallorenlargemodels/gru_cache.pt\n  Loaded from: 2026-01-24T21:44:02.488192\n  GRU OOF shape: (3043,)\n  GRU test shape: (7135,)\n\n  RAW GRU AUC: 0.4888\n  RAW GRU - TDE mean: 0.1510\n  RAW GRU - Non-TDE mean: 0.1569\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Start fresh - just Bhardwaj features\ntrain_df = train_feats[['object_id'] + [c for c in BHARDWAJ_FEATURES if c in train_feats.columns]].copy()\ntest_df = test_feats[['object_id'] + [c for c in BHARDWAJ_FEATURES if c in test_feats.columns]].copy()\n# Add target to train\ntrain_df = train_df.merge(train_log[['object_id', 'target']], on='object_id', how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:22:02.489748Z","iopub.execute_input":"2026-01-31T09:22:02.490497Z","iopub.status.idle":"2026-01-31T09:22:02.515535Z","shell.execute_reply.started":"2026-01-31T09:22:02.490449Z","shell.execute_reply":"2026-01-31T09:22:02.514484Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#The GRU was stored in a different order, here we fix that.\n#GRU predictions are in alphabetical order of object_id\n# Check if main notebook order is alphabetically sorted\nmain_first_10 = ['Dornhoth_anwar_melethron', 'Dornhoth_archam_grond', 'Dornhoth_certh_iaun', \n                 'Dornhoth_drafn_celon', 'Dornhoth_fervain_onodrim', 'Dornhoth_galadh_ylf', \n                 'Dornhoth_gwend_nagol', 'Dornhoth_hervenn_tathar', 'Dornhoth_inias_gond', \n                 'Dornhoth_lavan_ank']\n\nsorted_ids = sorted(train_log['object_id'].tolist())\nprint(f\"Sorted first 10: {sorted_ids[:10]}\")\nprint(f\"Main notebook first 10 matches sorted: {main_first_10 == sorted_ids[:10]}\")\n\n# Create mapping\ngru_order = sorted(train_log['object_id'].tolist())\ngru_oof_dict = dict(zip(gru_order, gru_oof_raw))\n\ngru_test_order = sorted(test_log['object_id'].tolist())\ngru_test_dict = dict(zip(gru_test_order, gru_test_raw))\n\n# Map to Bhardwaj train_df order\ngru_oof_aligned = np.array([gru_oof_dict[oid] for oid in train_df['object_id']])\ngru_test_aligned = np.array([gru_test_dict[oid] for oid in test_df['object_id']])\n\n# Validate\ny_check = train_df['target'].values\nauc_aligned = roc_auc_score(y_check, gru_oof_aligned)\nprint(f\"\\n*** Aligned GRU AUC: {auc_aligned:.4f} ***\")\nprint(f\"TDE mean: {gru_oof_aligned[y_check==1].mean():.4f}\")\nprint(f\"Non-TDE mean: {gru_oof_aligned[y_check==0].mean():.4f}\")\n\nif auc_aligned > 0.9:\n    gru_oof_raw = gru_oof_aligned  # Replace with aligned version\n    gru_test_raw = gru_test_aligned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:35:34.811288Z","iopub.execute_input":"2026-01-31T09:35:34.811971Z","iopub.status.idle":"2026-01-31T09:35:34.831886Z","shell.execute_reply.started":"2026-01-31T09:35:34.811942Z","shell.execute_reply":"2026-01-31T09:35:34.830947Z"}},"outputs":[{"name":"stdout","text":"Sorted first 10: ['Dornhoth_anwar_melethron', 'Dornhoth_archam_grond', 'Dornhoth_certh_iaun', 'Dornhoth_drafn_celon', 'Dornhoth_fervain_onodrim', 'Dornhoth_galadh_ylf', 'Dornhoth_gwend_nagol', 'Dornhoth_hervenn_tathar', 'Dornhoth_inias_gond', 'Dornhoth_lavan_ank']\nMain notebook first 10 matches sorted: True\n\n*** Aligned GRU AUC: 0.9544 ***\nTDE mean: 0.8013\nNon-TDE mean: 0.1236\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Platt scaling\ny_for_platt = train_df['target'].values\nlr = LogisticRegression(C=1e10, solver='lbfgs', max_iter=1000)\nlr.fit(gru_oof_aligned.reshape(-1, 1), y_for_platt)\ngru_oof_platt = lr.predict_proba(gru_oof_aligned.reshape(-1, 1))[:, 1]\ngru_test_platt = lr.predict_proba(gru_test_aligned.reshape(-1, 1))[:, 1]\n\nprint(f\"Platt coef: {lr.coef_[0][0]:.3f}, intercept: {lr.intercept_[0]:.3f}\")\nprint(f\"Platt-scaled AUC: {roc_auc_score(y_for_platt, gru_oof_platt):.4f}\")\n\n# Add to train_df and test_df\ntrain_df['gru_pred'] = gru_oof_platt\ntest_df['gru_pred'] = gru_test_platt\n\n# Update feature columns\nFEATURE_COLS_WITH_GRU = BHARDWAJ_FEATURES + ['gru_pred']\nprint(f\"\\nFeatures: {FEATURE_COLS_WITH_GRU}\")\n\n# Prepare training data\nX_train = train_df[FEATURE_COLS_WITH_GRU].fillna(-999)\ny_train = train_df['target']\nX_test = test_df[FEATURE_COLS_WITH_GRU].fillna(-999)\n\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"X_test: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:35:42.055477Z","iopub.execute_input":"2026-01-31T09:35:42.056494Z","iopub.status.idle":"2026-01-31T09:35:42.079933Z","shell.execute_reply.started":"2026-01-31T09:35:42.056450Z","shell.execute_reply":"2026-01-31T09:35:42.078976Z"}},"outputs":[{"name":"stdout","text":"Platt coef: 6.277, intercept: -5.818\nPlatt-scaled AUC: 0.9544\n\nFeatures: ['Amplitude', 'LengthScale_Time', 'LengthScale_Wavelength', 'Rise_Time', 'Fade_Time', 'Mean_Color_Pre_gr', 'Mean_Color_Post_gr', 'Mean_Color_Pre_ri', 'Mean_Color_Post_ri', 'Slope_Pre_gr', 'Slope_Post_gr', 'Slope_Pre_ri', 'Slope_Post_ri', 'gru_pred']\nX_train: (3043, 14)\nX_test: (7135, 14)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"TRANSFORMER_CACHE_PATH = Path(\"/kaggle/input/mallorenlargemodels/transformer_cache.pt\")\n\nif TRANSFORMER_CACHE_PATH.exists():\n    trans_cache = torch.load(TRANSFORMER_CACHE_PATH, map_location='cpu', weights_only=False)\n    trans_oof_raw = trans_cache['oof']\n    trans_test_raw = trans_cache['test']\n    \n    print(f\"  Loaded: {trans_cache.get('timestamp')}\")\n    \n    # Align by sorted order (same fix as GRU)\n    trans_order = sorted(train_log['object_id'].tolist())\n    trans_oof_dict = dict(zip(trans_order, trans_oof_raw))\n    \n    trans_test_order = sorted(test_log['object_id'].tolist())\n    trans_test_dict = dict(zip(trans_test_order, trans_test_raw))\n    \n    # Map to train_df/test_df order\n    trans_oof_aligned = np.array([trans_oof_dict[oid] for oid in train_df['object_id']])\n    trans_test_aligned = np.array([trans_test_dict[oid] for oid in test_df['object_id']])\n    \n    # Validate\n    y_check = train_df['target'].values\n    auc_trans = roc_auc_score(y_check, trans_oof_aligned)\n    print(f\"  Aligned Transformer AUC: {auc_trans:.4f}\")\n    \n    if auc_trans > 0.8:\n        print(\"  ✓ Transformer aligned correctly!\")\n        \n        # Platt scale\n        lr_trans = LogisticRegression(C=1e10, solver='lbfgs', max_iter=1000)\n        lr_trans.fit(trans_oof_aligned.reshape(-1, 1), y_check)\n        trans_oof_platt = lr_trans.predict_proba(trans_oof_aligned.reshape(-1, 1))[:, 1]\n        trans_test_platt = lr_trans.predict_proba(trans_test_aligned.reshape(-1, 1))[:, 1]\n        \n        train_df['transformer_pred'] = trans_oof_platt\n        test_df['transformer_pred'] = trans_test_platt\n        print(f\"  Added transformer_pred\")\n    else:\n        print(f\"  ✗ Transformer AUC too low ({auc_trans:.4f}), skipping\")\nelse:\n    print(f\"  Transformer cache not found: {TRANSFORMER_CACHE_PATH}\")\n\n\n#GP\nprint(\"\\n2. Loading GP features...\")\n\nGP_CACHE_DIR = Path(\"/kaggle/input/gpfeatures/gp_cache/\")\nTRAIN_GP_CACHE = GP_CACHE_DIR / \"train_gp_features_v11.parquet\"\nTEST_GP_CACHE = GP_CACHE_DIR / \"test_gp_features_v11.parquet\"\n\nif TRAIN_GP_CACHE.exists() and TEST_GP_CACHE.exists():\n    train_gp = pd.read_parquet(TRAIN_GP_CACHE)\n    test_gp = pd.read_parquet(TEST_GP_CACHE)\n    print(f\"  Loaded train GP: {train_gp.shape}\")\n    print(f\"  Loaded test GP: {test_gp.shape}\")\n    \n    # Key GP features\n    GP_FEATURES = [\n        'gp_decay_slope_30d', 'gp_decay_slope_60d', 'gp_decay_slope_100d',\n        'gp_decay_rate_norm_30d', 'gp_decay_rate_norm_60d',\n        'gp_decay_slope_r', 'gp_decay_slope_i', 'gp_decay_slope_z',\n        'gp_decay_r2_100d', 'gp_t_above_half',\n        'gp_color_gr_peak', 'gp_color_gr_30d', 'gp_color_evolution_gr',\n    ]\n    gp_cols = ['object_id'] + [c for c in GP_FEATURES if c in train_gp.columns]\n    \n    train_df = train_df.merge(train_gp[gp_cols], on='object_id', how='left')\n    test_df = test_df.merge(test_gp[gp_cols], on='object_id', how='left')\n    print(f\"  Added {len(gp_cols)-1} GP features\")\nelse:\n    print(f\"  GP cache not found: {TRAIN_GP_CACHE}\")\n\n\n#SNCOSMO FEATURES\nprint(\"\\n3. Loading SNCOSMO features...\")\n\nSNCOSMO_TRAIN_PATH = Path(\"/kaggle/input/sncosmos/train_sncosmo_features.parquet\")\nSNCOSMO_TEST_PATH = Path(\"/kaggle/input/sncosmos/test_sncosmo_features.parquet\")\n\nif SNCOSMO_TRAIN_PATH.exists() and SNCOSMO_TEST_PATH.exists():\n    sncosmo_train = pd.read_parquet(SNCOSMO_TRAIN_PATH)\n    sncosmo_test = pd.read_parquet(SNCOSMO_TEST_PATH)\n    print(f\"  Loaded train SNCOSMO: {sncosmo_train.shape}\")\n    print(f\"  Loaded test SNCOSMO: {sncosmo_test.shape}\")\n    \n    # Key SNCOSMO features\n    SNCOSMO_FEATURES = [\n        'sn_salt2_rchisq', 'sn_salt3_rchisq', \n        'sn_best_ia_rchisq', 'sn_is_good_ia_fit',\n        'sn_salt2_x1', 'sn_salt2_c',\n        'sn_best_cc_rchisq', 'sn_is_good_cc_fit',\n    ]\n    sncosmo_cols = ['object_id'] + [c for c in SNCOSMO_FEATURES if c in sncosmo_train.columns]\n    \n    train_df = train_df.merge(sncosmo_train[sncosmo_cols], on='object_id', how='left')\n    test_df = test_df.merge(sncosmo_test[sncosmo_cols], on='object_id', how='left')\n    print(f\"  Added {len(sncosmo_cols)-1} SNCOSMO features\")\nelse:\n    print(f\"  SNCOSMO not found: {SNCOSMO_TRAIN_PATH}\")\n\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\n\n# List all features (excluding object_id and target)\nall_features = [c for c in train_df.columns if c not in ['object_id', 'target']]\nprint(f\"\\nTotal features: {len(all_features)}\")\nprint(f\"Features: {all_features}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:23:38.121251Z","iopub.execute_input":"2026-01-31T09:23:38.122113Z","iopub.status.idle":"2026-01-31T09:23:38.671675Z","shell.execute_reply.started":"2026-01-31T09:23:38.122081Z","shell.execute_reply":"2026-01-31T09:23:38.670843Z"}},"outputs":[{"name":"stdout","text":"============================================================\nAdding additional features: Transformer, GP, SNCOSMO\n============================================================\n\n1. Loading Transformer predictions...\n  Loaded: 2026-01-24T23:20:26.479422\n  Aligned Transformer AUC: 0.8954\n  ✓ Transformer aligned correctly!\n  Added transformer_pred\n\n2. Loading GP features...\n  Loaded train GP: (3043, 25)\n  Loaded test GP: (7135, 25)\n  Added 13 GP features\n\n3. Loading SNCOSMO features...\n  Loaded train SNCOSMO: (3043, 44)\n  Loaded test SNCOSMO: (7135, 44)\n  Added 8 SNCOSMO features\n\n============================================================\nFEATURE SUMMARY\n============================================================\nTrain shape: (3043, 38)\nTest shape: (7135, 37)\n\nTotal features: 36\nFeatures: ['Amplitude', 'LengthScale_Time', 'LengthScale_Wavelength', 'Rise_Time', 'Fade_Time', 'Mean_Color_Pre_gr', 'Mean_Color_Post_gr', 'Mean_Color_Pre_ri', 'Mean_Color_Post_ri', 'Slope_Pre_gr', 'Slope_Post_gr', 'Slope_Pre_ri', 'Slope_Post_ri', 'gru_pred', 'transformer_pred', 'gp_decay_slope_30d', 'gp_decay_slope_60d', 'gp_decay_slope_100d', 'gp_decay_rate_norm_30d', 'gp_decay_rate_norm_60d', 'gp_decay_slope_r', 'gp_decay_slope_i', 'gp_decay_slope_z', 'gp_decay_r2_100d', 'gp_t_above_half', 'gp_color_gr_peak', 'gp_color_gr_30d', 'gp_color_evolution_gr', 'sn_salt2_rchisq', 'sn_salt3_rchisq', 'sn_best_ia_rchisq', 'sn_is_good_ia_fit', 'sn_salt2_x1', 'sn_salt2_c', 'sn_best_cc_rchisq', 'sn_is_good_cc_fit']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"FEATURE_COLS = [c for c in train_df.columns if c not in ['object_id', 'target']]\nprint(f\"Training with {len(FEATURE_COLS)} features\")\n\nX_train = train_df[FEATURE_COLS].fillna(-999)\ny_train = train_df['target']\nX_test = test_df[FEATURE_COLS].fillna(-999)\n\nyy = train_log.set_index(\"object_id\").loc[train_df[\"object_id\"], \"target\"].astype(int).values\npos = (yy == 1).sum()\nneg = (yy == 0).sum()\nscale_pos_weight = neg / pos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:26:04.313074Z","iopub.execute_input":"2026-01-31T09:26:04.313473Z","iopub.status.idle":"2026-01-31T09:26:04.330952Z","shell.execute_reply.started":"2026-01-31T09:26:04.313434Z","shell.execute_reply":"2026-01-31T09:26:04.329940Z"}},"outputs":[{"name":"stdout","text":"Training with 36 features\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Train XGBoost\nprint(\"Training XGBoost with 5-fold CV...\")\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds = np.zeros(len(X_train))\nmodels = []\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n\n    model = xgb.XGBClassifier(\n        n_estimators=9000,\n        learning_rate=0.005,\n        max_depth=6,\n        min_child_weight=2,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        scale_pos_weight=scale_pos_weight,\n        random_state=SEED,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        early_stopping_rounds=600\n    )\n\n    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n    oof_preds[va_idx] = model.predict_proba(X_va)[:, 1]\n    models.append(model)\n\n    # Fold metrics\n    f1_fold = max(f1_score(y_va, (oof_preds[va_idx] >= t).astype(int)) \n                  for t in np.linspace(0.2, 0.6, 41))\n    print(f\"  Fold {fold}: Best F1 = {f1_fold:.4f}\")\n# Overall results\nprint(\"\\n\" + \"=\"*50)\nbest_f1 = max(f1_score(y_train, (oof_preds >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nbest_thr = max(np.linspace(0.1, 0.7, 61), key=lambda t: f1_score(y_train, (oof_preds >= t).astype(int)))\nprint(f\"Overall CV F1: {best_f1:.4f}\")\nprint(f\"Best threshold: {best_thr:.2f}\")\noof_binary = (oof_preds >= best_thr).astype(int)\nprint(f\"Precision: {precision_score(y_train, oof_binary):.4f}\")\nprint(f\"Recall: {recall_score(y_train, oof_binary):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_train, oof_binary)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:26:07.711483Z","iopub.execute_input":"2026-01-31T09:26:07.712133Z","iopub.status.idle":"2026-01-31T09:26:33.182339Z","shell.execute_reply.started":"2026-01-31T09:26:07.712101Z","shell.execute_reply":"2026-01-31T09:26:33.181551Z"}},"outputs":[{"name":"stdout","text":"Training XGBoost with 5-fold CV...\n  Fold 1: Best F1 = 0.6486\n  Fold 2: Best F1 = 0.7222\n  Fold 3: Best F1 = 0.7143\n  Fold 4: Best F1 = 0.7105\n  Fold 5: Best F1 = 0.6377\n\n==================================================\nOverall CV F1: 0.6703\nBest threshold: 0.33\nPrecision: 0.5586\nRecall: 0.8378\n\nConfusion Matrix:\n[[2797   98]\n [  24  124]]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Feature importance\nimportance = pd.DataFrame({\n    'feature': FEATURE_COLS,\n    'importance': np.mean([m.feature_importances_ for m in models], axis=0)\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 features:\")\nprint(importance.head(28).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:27:10.652019Z","iopub.execute_input":"2026-01-31T09:27:10.653040Z","iopub.status.idle":"2026-01-31T09:27:10.682262Z","shell.execute_reply.started":"2026-01-31T09:27:10.653007Z","shell.execute_reply":"2026-01-31T09:27:10.681356Z"}},"outputs":[{"name":"stdout","text":"\nTop 10 features:\n               feature  importance\n              gru_pred    0.231942\n      transformer_pred    0.083128\nLengthScale_Wavelength    0.068478\n     sn_is_good_ia_fit    0.048104\n          Slope_Pre_gr    0.036335\n         Slope_Post_gr    0.035469\n       gp_t_above_half    0.034299\n     sn_best_ia_rchisq    0.030374\n             Fade_Time    0.026851\n     sn_is_good_cc_fit    0.025484\n             Rise_Time    0.025136\n      LengthScale_Time    0.021514\n    Mean_Color_Post_gr    0.020135\n       sn_salt2_rchisq    0.018963\n          Slope_Pre_ri    0.018386\n   gp_decay_slope_100d    0.017887\n      gp_decay_slope_r    0.017763\n         Slope_Post_ri    0.017271\n    gp_decay_slope_60d    0.015894\n     Mean_Color_Pre_gr    0.015479\n           sn_salt2_x1    0.015009\n      gp_decay_r2_100d    0.014572\n       sn_salt3_rchisq    0.014556\n gp_color_evolution_gr    0.014060\n    gp_decay_slope_30d    0.013244\ngp_decay_rate_norm_60d    0.012958\n      gp_decay_slope_i    0.012552\n            sn_salt2_c    0.012194\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\nN_FOLDS = 3\nN_SEEDS = 3 \n\n# Store OOF predictions\noof_xgb = np.zeros(len(X_train), dtype=np.float64)\noof_lgb = np.zeros(len(X_train), dtype=np.float64)\noof_cat = np.zeros(len(X_train), dtype=np.float64)\n\ntest_xgb = np.zeros(len(X_test), dtype=np.float64)\ntest_lgb = np.zeros(len(X_test), dtype=np.float64)\ntest_cat = np.zeros(len(X_test), dtype=np.float64)\n\nfor seed_idx in range(N_SEEDS):\n    seed = SEED + seed_idx * 42\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n    \n    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_train, y_train), 1):\n        X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n        y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]\n        \n        # XGBoost\n        xgb_model = xgb.XGBClassifier(\n            n_estimators=2000, learning_rate=0.03, max_depth=6,\n            min_child_weight=2, subsample=0.8, colsample_bytree=0.6,\n            gamma=0.1, reg_lambda=2, random_state=seed,\n            eval_metric='aucpr', early_stopping_rounds=150, verbosity=0\n        )\n        xgb_model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n        oof_xgb[va_idx] += xgb_model.predict_proba(X_va)[:, 1] / N_SEEDS\n        test_xgb += xgb_model.predict_proba(X_test)[:, 1] / (N_FOLDS * N_SEEDS)\n        \n        # LightGBM\n        lgb_model = lgb.LGBMClassifier(\n            n_estimators=2000, learning_rate=0.03, num_leaves=16,\n            max_depth=-1, min_child_samples=20, subsample=0.8,\n            colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=2,\n            random_state=seed, verbosity=-1\n        )\n        lgb_model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n                      callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(0)])\n        oof_lgb[va_idx] += lgb_model.predict_proba(X_va)[:, 1] / N_SEEDS\n        test_lgb += lgb_model.predict_proba(X_test)[:, 1] / (N_FOLDS * N_SEEDS)\n        \n        # CatBoost\n        cat_model = CatBoostClassifier(\n            iterations=2000, learning_rate=0.03, depth=6,\n            l2_leaf_reg=6, subsample=0.8, colsample_bylevel=0.6,\n            early_stopping_rounds=150, random_seed=seed, verbose=False\n        )\n        cat_model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n        oof_cat[va_idx] += cat_model.predict_proba(X_va)[:, 1] / N_SEEDS\n        test_cat += cat_model.predict_proba(X_test)[:, 1] / (N_FOLDS * N_SEEDS)\n    \n    print(f\"  Seed {seed_idx+1}/{N_SEEDS} done\")\n\n# Individual scores\n\nfor name, oof in [(\"XGB\", oof_xgb), (\"LGB\", oof_lgb), (\"CAT\", oof_cat)]:\n    f1 = max(f1_score(y_train, (oof >= t).astype(int)) for t in np.linspace(0.05, 0.6, 56))\n    print(f\"  {name}: F1 = {f1:.4f}\")\n\n# Find best blend\nbest_f1 = 0\nbest_weights = (1/3, 1/3, 1/3)\nbest_thr = 0.2\n\nfor w1 in np.arange(0.0, 0.9, 0.1):\n    for w2 in np.arange(0.0, 0.9 - w1, 0.1):\n        w3 = 1 - w1 - w2\n        if w3 < 0.05:\n            continue\n        blend = w1 * oof_xgb + w2 * oof_lgb + w3 * oof_cat\n        for thr in np.linspace(0.05, 0.5, 46):\n            f1 = f1_score(y_train, (blend >= thr).astype(int))\n            if f1 > best_f1:\n                best_f1 = f1\n                best_weights = (w1, w2, w3)\n                best_thr = thr\n\nprint(f\"\\nBest blend: XGB={best_weights[0]:.1f}, LGB={best_weights[1]:.1f}, CAT={best_weights[2]:.1f}\")\nprint(f\"Best threshold: {best_thr:.2f}\")\nprint(f\"Best CV F1: {best_f1:.4f}\")\n\n# Final blend\noof_blend = best_weights[0] * oof_xgb + best_weights[1] * oof_lgb + best_weights[2] * oof_cat\ntest_blend = best_weights[0] * test_xgb + best_weights[1] * test_lgb + best_weights[2] * test_cat\n\noof_binary = (oof_blend >= best_thr).astype(int)\nprint(f\"\\nPrecision: {precision_score(y_train, oof_binary):.4f}\")\nprint(f\"Recall: {recall_score(y_train, oof_binary):.4f}\")\nprint(f\"Predicted: {oof_binary.sum()} / {y_train.sum()} actual\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:37:12.851058Z","iopub.execute_input":"2026-01-31T09:37:12.851373Z","iopub.status.idle":"2026-01-31T09:37:32.795836Z","shell.execute_reply.started":"2026-01-31T09:37:12.851348Z","shell.execute_reply":"2026-01-31T09:37:32.794901Z"}},"outputs":[{"name":"stdout","text":"  Seed 1/3 done\n  Seed 2/3 done\n  Seed 3/3 done\n  XGB: F1 = 0.6630\n  LGB: F1 = 0.6630\n  CAT: F1 = 0.6705\n\nBest blend: XGB=0.4, LGB=0.0, CAT=0.6\nBest threshold: 0.21\nBest CV F1: 0.6744\n\nPrecision: 0.5879\nRecall: 0.7905\nPredicted: 199 / 148 actual\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Apply threshold to get binary predictions\ntest_preds = (test_blend >= best_thr).astype(int)\n\n# Create submission dataframe\nsubmission = pd.DataFrame({\n    'object_id': test_df['object_id'], \n    'target': test_preds\n})\n\n# Save\nsubmission.to_csv('submission.csv', index=False)\n\nprint(f\"Saved submission.csv\")\nprint(f\"Threshold: {best_thr:.2f}\")\nprint(f\"Predicted TDEs: {test_preds.sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T09:37:32.797261Z","iopub.execute_input":"2026-01-31T09:37:32.797599Z","iopub.status.idle":"2026-01-31T09:37:32.814262Z","shell.execute_reply.started":"2026-01-31T09:37:32.797574Z","shell.execute_reply":"2026-01-31T09:37:32.813269Z"}},"outputs":[{"name":"stdout","text":"Saved submission.csv\nThreshold: 0.21\nPredicted TDEs: 422\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}