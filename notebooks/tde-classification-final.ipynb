{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14436762,"sourceType":"datasetVersion","datasetId":9221482},{"sourceId":14610868,"sourceType":"datasetVersion","datasetId":9332666},{"sourceId":14611013,"sourceType":"datasetVersion","datasetId":9284963,"isSourceIdPinned":true},{"sourceId":14637655,"sourceType":"datasetVersion","datasetId":9350625},{"sourceId":14678819,"sourceType":"datasetVersion","datasetId":9377747}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":10716.729482,"end_time":"2026-01-22T19:21:06.365296","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-22T16:22:29.635814","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TDE Classification Pipeline v11\n\nCleaned version with TOP 100 features and parallel processing.\n","metadata":{"papermill":{"duration":0.005695,"end_time":"2026-01-22T16:22:32.142911","exception":false,"start_time":"2026-01-22T16:22:32.137216","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## SECTION 1: IMPORTS AND CONFIGURATION","metadata":{"papermill":{"duration":0.004406,"end_time":"2026-01-22T16:22:32.171498","exception":false,"start_time":"2026-01-22T16:22:32.167092","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import rankdata, pearsonr\nfrom scipy.ndimage import gaussian_filter1d\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    f1_score, precision_score, recall_score,\n    roc_auc_score, average_precision_score, confusion_matrix\n)\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nfrom joblib import Parallel, delayed\n\ntry:\n    from numba import jit\n    HAS_NUMBA = True\nexcept ImportError:\n    HAS_NUMBA = False\n    def jit(*args, **kwargs):\n        def decorator(func):\n            return func\n        return decorator\n\n# Configuration\nSEED = 42\nN_FOLDS = 5\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSIGMA_THRESHOLD = 1.0  # Detection threshold\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(SEED)\n\n\nFEATURES_TO_REMOVE = []\n\n\n# Model caching configuration\nCACHE_DIR = Path(\"/kaggle/input/mallorenlargemodels\")#Path(\"/kaggle/working/\")\nCACHE_DIR.mkdir(exist_ok=True)\nUSE_CACHED_SEQ_MODELS = True  # Set False to force retraining\nSAVE_SEQ_MODELS = True\n\n# SNCOSMO features path\nSNCOSMO_TRAIN_PATH = Path(\"/kaggle/input/sncosmos/train_sncosmo_features.parquet\")\nSNCOSMO_TEST_PATH = Path(\"/kaggle/input/sncosmos/test_sncosmo_features.parquet\")\n# Fallback to working directory\nif not SNCOSMO_TRAIN_PATH.exists():\n    SNCOSMO_TRAIN_PATH = Path(\"/kaggle/working/train_sncosmo_features.parquet\")\n    SNCOSMO_TEST_PATH = Path(\"/kaggle/working/test_sncosmo_features.parquet\")\n\n# SN Ia Veto\nUSE_SNIA_VETO = True  # Apply hard veto for objects with good SN Ia fits\n\nprint(f\"Use SN Ia veto: {USE_SNIA_VETO}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:58:28.839092Z","iopub.execute_input":"2026-01-31T10:58:28.839454Z","iopub.status.idle":"2026-01-31T10:58:43.690369Z","shell.execute_reply.started":"2026-01-31T10:58:28.839421Z","shell.execute_reply":"2026-01-31T10:58:43.689463Z"},"papermill":{"duration":12.441422,"end_time":"2026-01-22T16:22:44.617379","exception":false,"start_time":"2026-01-22T16:22:32.175957","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Use SN Ia veto: True\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## SECTION 2: DATA LOADING","metadata":{"papermill":{"duration":0.004435,"end_time":"2026-01-22T16:22:44.626681","exception":false,"start_time":"2026-01-22T16:22:44.622246","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DATASET_DIR = Path(\"/kaggle/input/mallorn-astronomical-classification-challenge\")\n\ntrain_log = pd.read_csv(DATASET_DIR / \"train_log.csv\")\ntest_log = pd.read_csv(DATASET_DIR / \"test_log.csv\")\n\nsplit_dirs = sorted([p for p in DATASET_DIR.glob(\"split_*\") if p.is_dir()])\n\ntrain_lc = pd.concat([pd.read_csv(d / \"train_full_lightcurves.csv\") for d in split_dirs], ignore_index=True)\ntest_lc = pd.concat([pd.read_csv(d / \"test_full_lightcurves.csv\") for d in split_dirs], ignore_index=True)\n\nfor df in (train_log, test_log):\n    if \"Z_err\" in df.columns:\n        df[\"Z_err\"] = pd.to_numeric(df[\"Z_err\"], errors=\"coerce\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:58:43.691899Z","iopub.execute_input":"2026-01-31T10:58:43.692577Z","iopub.status.idle":"2026-01-31T10:58:46.292201Z","shell.execute_reply.started":"2026-01-31T10:58:43.692545Z","shell.execute_reply":"2026-01-31T10:58:46.291252Z"},"papermill":{"duration":2.269385,"end_time":"2026-01-22T16:22:46.900483","exception":false,"start_time":"2026-01-22T16:22:44.631098","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## SECTION 3: PREPROCESSING","metadata":{"papermill":{"duration":0.004665,"end_time":"2026-01-22T16:22:46.910931","exception":false,"start_time":"2026-01-22T16:22:46.906266","status":"completed"},"tags":[]}},{"cell_type":"code","source":"FILTERS = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\nf2i = {f: i for i, f in enumerate(FILTERS)}\n\ndef prep_lightcurves(df):\n    df = df.copy()\n    df[\"Filter\"] = df[\"Filter\"].astype(str).str.strip().str.lower()\n    df[\"filter_id\"] = df[\"Filter\"].map(f2i).astype(\"int64\")\n    df[\"Time (MJD)\"] = df[\"Time (MJD)\"].astype(\"float64\")\n    df[\"Flux\"] = df[\"Flux\"].astype(\"float64\")\n    df[\"Flux_err\"] = df[\"Flux_err\"].astype(\"float64\")\n    return df\n\ntrain_lc = prep_lightcurves(train_lc)\ntest_lc = prep_lightcurves(test_lc)\n\n# # Fixed:\ntrain_lc = train_lc.merge(train_log[[\"object_id\", \"EBV\", \"Z\", \"target\"]], on=\"object_id\", how=\"left\")\ntest_lc = test_lc.merge(test_log[[\"object_id\", \"EBV\", \"Z\"]], on=\"object_id\", how=\"left\")\n\nprint(\"Preprocessed lightcurves.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:58:46.293292Z","iopub.execute_input":"2026-01-31T10:58:46.293601Z","iopub.status.idle":"2026-01-31T10:58:47.551220Z","shell.execute_reply.started":"2026-01-31T10:58:46.293571Z","shell.execute_reply":"2026-01-31T10:58:47.550235Z"},"papermill":{"duration":1.04206,"end_time":"2026-01-22T16:22:47.957546","exception":false,"start_time":"2026-01-22T16:22:46.915486","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Preprocessed lightcurves.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## SECTION 4: EXTINCTION CORRECTION","metadata":{"papermill":{"duration":0.004667,"end_time":"2026-01-22T16:22:47.967221","exception":false,"start_time":"2026-01-22T16:22:47.962554","status":"completed"},"tags":[]}},{"cell_type":"code","source":"try:\n    from extinction import fitzpatrick99\nexcept ImportError:\n    import subprocess\n    subprocess.run([\"pip\", \"-q\", \"install\", \"extinction==0.4.7\"], check=True)\n    from extinction import fitzpatrick99\n\nEFF_WL = {\n    \"u\": 3641.0, \"g\": 4704.0, \"r\": 6155.0,\n    \"i\": 7504.0, \"z\": 8695.0, \"y\": 10056.0,\n}\n\ndef apply_deextinction(df):\n    df = df.copy()\n    df[\"Flux_corr\"] = df[\"Flux\"].copy()\n    df[\"Fluxerr_corr\"] = df[\"Flux_err\"].copy()\n    \n    for filt in df[\"Filter\"].unique():\n        if filt not in EFF_WL:\n            continue\n        filt_mask = df[\"Filter\"] == filt\n        wl_arr = np.array([EFF_WL[filt]])\n        \n        for ebv_val in df.loc[filt_mask, \"EBV\"].unique():\n            if pd.isna(ebv_val) or ebv_val == 0:\n                continue\n            mask = filt_mask & (df[\"EBV\"] == ebv_val)\n            A_lambda = fitzpatrick99(wl_arr, ebv_val * 3.1)[0]\n            factor = 10 ** (A_lambda / 2.5)\n            df.loc[mask, \"Flux_corr\"] = df.loc[mask, \"Flux\"] * factor\n            df.loc[mask, \"Fluxerr_corr\"] = df.loc[mask, \"Flux_err\"] * factor\n    \n    return df\n\nprint(\"Applying extinction correction...\")\ntrain_lc = apply_deextinction(train_lc)\ntest_lc = apply_deextinction(test_lc)\nprint(\"Done.\")\n\ndef drop_bad_rows(lc, name=\"\"):\n    \"\"\"Only drop rows with NaN/inf values. Keep negative flux.\"\"\"\n    before = len(lc)\n    keep = (\n        np.isfinite(lc[\"Time (MJD)\"].values) &\n        np.isfinite(lc[\"Flux_corr\"].values) &\n        np.isfinite(lc[\"Fluxerr_corr\"].values)\n    )\n    lc = lc[keep].copy()\n    print(f\"{name}: Dropped {before - len(lc):,} bad rows (NaN/inf only)\")\n    return lc\n\ntrain_lc = drop_bad_rows(train_lc, \"Train\")\ntest_lc = drop_bad_rows(test_lc, \"Test\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:58:47.553076Z","iopub.execute_input":"2026-01-31T10:58:47.553372Z","iopub.status.idle":"2026-01-31T10:59:11.892095Z","shell.execute_reply.started":"2026-01-31T10:58:47.553346Z","shell.execute_reply":"2026-01-31T10:59:11.891318Z"},"papermill":{"duration":19.465551,"end_time":"2026-01-22T16:23:07.437357","exception":false,"start_time":"2026-01-22T16:22:47.971806","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 582.9/582.9 kB 9.9 MB/s eta 0:00:00\nApplying extinction correction...\nDone.\nTrain: Dropped 891 bad rows (NaN/inf only)\nTest: Dropped 2,022 bad rows (NaN/inf only)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## SECTION 5: 3-SIGMA BASELINE DETECTION","metadata":{"papermill":{"duration":0.004965,"end_time":"2026-01-22T16:23:07.447503","exception":false,"start_time":"2026-01-22T16:23:07.442538","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def add_detection_columns_fast(lc, sigma_threshold=3.0):\n    \"\"\"Add detection columns - vectorized version (5-10x faster).\"\"\"\n    lc = lc.copy()\n    \n    # Compute per-object statistics in one groupby pass\n    def compute_group_stats(g):\n        flux = g['Flux_corr'].values\n        flux_err = g['Fluxerr_corr'].values\n        n = len(flux)\n        \n        if n < 3:\n            return pd.Series({'baseline': 0.0, 'noise': 1.0})\n        \n        baseline = np.percentile(flux, 25)\n        noise_from_err = np.median(np.abs(flux_err))\n        noise_from_mad = 1.4826 * np.median(np.abs(flux - np.median(flux)))\n        noise = max(noise_from_err, noise_from_mad, 1e-10)\n        \n        return pd.Series({'baseline': baseline, 'noise': noise})\n    \n    # Single groupby operation\n    stats = lc.groupby('object_id', sort=False).apply(compute_group_stats, include_groups=False)\n    \n    # Map stats back to original rows (fast)\n    lc['baseline'] = lc['object_id'].map(stats['baseline'])\n    lc['noise'] = lc['object_id'].map(stats['noise'])\n    \n    # Vectorized computation on full dataframe (very fast)\n    lc['deviation'] = (lc['Flux_corr'] - lc['baseline']) / lc['noise']\n    lc['is_detected'] = lc['deviation'] > sigma_threshold\n    \n    # Clean up\n    lc.drop(columns=['noise'], inplace=True)\n    \n    return lc\n\nprint(f\"\\nComputing {SIGMA_THRESHOLD}-sigma detections...\")\ntrain_lc = add_detection_columns_fast(train_lc, SIGMA_THRESHOLD)\ntest_lc = add_detection_columns_fast(test_lc, SIGMA_THRESHOLD)\n\n# Statistics\nprint(f\"Train detection rate: {train_lc['is_detected'].mean():.1%}\")\nprint(f\"Test detection rate: {test_lc['is_detected'].mean():.1%}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:59:11.893329Z","iopub.execute_input":"2026-01-31T10:59:11.893973Z","iopub.status.idle":"2026-01-31T10:59:17.911453Z","shell.execute_reply.started":"2026-01-31T10:59:11.893942Z","shell.execute_reply":"2026-01-31T10:59:17.910461Z"},"papermill":{"duration":650.499268,"end_time":"2026-01-22T16:33:57.951656","exception":false,"start_time":"2026-01-22T16:23:07.452388","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nComputing 1.0-sigma detections...\nTrain detection rate: 36.3%\nTest detection rate: 36.3%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## SECTION 6: FEATURE ENGINEERING","metadata":{"papermill":{"duration":0.004908,"end_time":"2026-01-22T16:33:57.961717","exception":false,"start_time":"2026-01-22T16:33:57.956809","status":"completed"},"tags":[]}},{"cell_type":"code","source":"LSST_LAMBDA_NM = {'u': 367.0, 'g': 482.0, 'r': 622.0, 'i': 754.0, 'z': 869.0, 'y': 971.0}\n\nif HAS_NUMBA:\n    @jit(nopython=True, cache=True)\n    def _safe_mad_jit(x):\n        if len(x) == 0:\n            return 0.0\n        med = np.median(x)\n        return float(np.median(np.abs(x - med)))\n    \n    @jit(nopython=True, cache=True)\n    def _von_neumann_jit(x):\n        n = len(x)\n        if n < 3:\n            return 0.0\n        diff_sum = 0.0\n        for i in range(n - 1):\n            diff_sum += (x[i+1] - x[i]) ** 2\n        mean = np.sum(x) / n\n        var = np.sum((x - mean) ** 2) / (n - 1)\n        return float(diff_sum / n) / (var + 1e-12)\n    \n    @jit(nopython=True, cache=True)  \n    def _longest_decline_jit(flux):\n        n = len(flux)\n        if n < 2:\n            return 0\n        max_run = current_run = 0\n        for i in range(n - 1):\n            if flux[i+1] < flux[i]:\n                current_run += 1\n                if current_run > max_run:\n                    max_run = current_run\n            else:\n                current_run = 0\n        return max_run\n\ndef _safe_mad(x):\n    if len(x) == 0:\n        return 0.0\n    if HAS_NUMBA:\n        return _safe_mad_jit(np.asarray(x, dtype=np.float64))\n    return float(np.median(np.abs(x - np.median(x))))\n\ndef _von_neumann_ratio(x):\n    if len(x) < 3:\n        return 0.0\n    if HAS_NUMBA:\n        return _von_neumann_jit(np.asarray(x, dtype=np.float64))\n    dif = np.diff(x)\n    v = np.var(x, ddof=1)\n    return float(np.mean(dif**2) / (v + 1e-12))\n\ndef _powerlaw_decay_index(t, f, t_peak, is_detected, min_points=4):\n    m = (t > t_peak) & (f > 0) & is_detected\n    tt, ff = t[m] - t_peak, f[m]\n    m2 = tt > 1e-3\n    tt, ff = tt[m2], ff[m2]\n    if len(tt) < min_points:\n        return 0.0, 0.0\n    try:\n        slope, _, r, _, _ = stats.linregress(np.log(tt), np.log(ff))\n        return float(np.clip(-slope, 0, 5)), float(r**2)\n    except:\n        return 0.0, 0.0\n\ndef _lin_slope(tt, yy):\n    if len(tt) < 3 or (tt.max() - tt.min()) <= 0:\n        return 0.0, 0.0\n    s, _, r, _, _ = stats.linregress(tt, yy)\n    return float(s), float(r**2)\n\ndef _longest_monotonic_run(flux, direction='decline'):\n    if len(flux) < 2:\n        return 0\n    if direction == 'decline' and HAS_NUMBA:\n        return _longest_decline_jit(np.asarray(flux, dtype=np.float64))\n    diff = np.diff(flux) < 0 if direction == 'decline' else np.diff(flux) > 0\n    max_run = current_run = 0\n    for d in diff:\n        if d:\n            current_run += 1\n            max_run = max(max_run, current_run)\n        else:\n            current_run = 0\n    return int(max_run)\n\n# Transient feature extraction\ndef find_transient_peak(obj_lc, z):\n    result = {'has_transient': False, 't_peak': None, 'detections': None}\n    all_det = []\n    for filt in FILTERS:\n        filt_lc = obj_lc[obj_lc['Filter'] == filt].copy()\n        if len(filt_lc) < 3:\n            continue\n        flux = filt_lc['Flux_corr'].values\n        flux_err = np.abs(filt_lc['Fluxerr_corr'].values) + 1e-10\n        baseline = np.percentile(flux, 25)\n        mad = 1.4826 * np.median(np.abs(flux - np.median(flux)))\n        noise = max(mad, np.median(flux_err), 1e-10)\n        snr = (flux - baseline) / noise\n        rel_err = flux_err / (np.abs(flux) + 1e-10)\n        good = (snr > 2.5) & (rel_err < 0.5) & (flux > 0)\n        if good.sum() > 0:\n            det = filt_lc[good].copy()\n            det['filter'] = filt\n            all_det.append(det)\n    if not all_det:\n        return result\n    detections = pd.concat(all_det, ignore_index=True)\n    if len(detections) < 4:\n        return result\n    for filt in ['r', 'g', 'i', 'z']:\n        filt_det = detections[detections['filter'] == filt]\n        if len(filt_det) > 0:\n            peak_idx = filt_det['Flux_corr'].idxmax()\n            result['t_peak'] = filt_det.loc[peak_idx, 'Time (MJD)']\n            result['has_transient'] = True\n            result['detections'] = detections\n            break\n    return result\n\ndef extract_transient_features(obj_lc, z):\n    features = {'trans_delta_m15': np.nan, 'trans_delta_m50': np.nan, 'trans_fade_rate': np.nan}\n    trans = find_transient_peak(obj_lc, z)\n    if not trans['has_transient']:\n        return features\n    t_peak = trans['t_peak']\n    detections = trans['detections']\n    z_factor = 1 + z if z > 0 else 1.0\n    \n    for band in ['r', 'g', 'i', 'z']:\n        band_det = detections[detections['filter'] == band]\n        if len(band_det) < 3:\n            continue\n        times = band_det['Time (MJD)'].values\n        fluxes = band_det['Flux_corr'].values\n        flux_errs = np.abs(band_det['Fluxerr_corr'].values) + 1e-10\n        t_rest = (times - t_peak) / z_factor\n        valid = (t_rest >= -50) & (t_rest <= 300)\n        if valid.sum() < 3:\n            continue\n        t_rest, fluxes, flux_errs = t_rest[valid], fluxes[valid], flux_errs[valid]\n        try:\n            from sklearn.gaussian_process import GaussianProcessRegressor\n            from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n            t_mean, t_std = t_rest.mean(), max(t_rest.std(), 1)\n            f_mean, f_std = fluxes.mean(), max(fluxes.std(), 1e-10)\n            t_norm = (t_rest - t_mean) / t_std\n            f_norm = (fluxes - f_mean) / f_std\n            err_norm = flux_errs / f_std\n            t_target = np.array([0, 15, 30, 50])\n            t_target_norm = (t_target - t_mean) / t_std\n            kernel = 1.0 * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(0.1)\n            gp = GaussianProcessRegressor(kernel=kernel, alpha=err_norm**2 + 0.01, \n                                          n_restarts_optimizer=1, random_state=42)\n            gp.fit(t_norm.reshape(-1, 1), f_norm)\n            f_pred = gp.predict(t_target_norm.reshape(-1, 1)) * f_std + f_mean\n            if f_pred[0] > 0:\n                mag_peak = -2.5 * np.log10(f_pred[0])\n                if f_pred[1] > 0:\n                    features['trans_delta_m15'] = -2.5 * np.log10(f_pred[1]) - mag_peak\n                if f_pred[3] > 0:\n                    features['trans_delta_m50'] = -2.5 * np.log10(f_pred[3]) - mag_peak\n                mags, t_vals = [], []\n                for j, t in enumerate([15, 30, 50]):\n                    if f_pred[j+1] > 0:\n                        mags.append(-2.5 * np.log10(f_pred[j+1]))\n                        t_vals.append(t)\n                if len(mags) >= 2:\n                    features['trans_fade_rate'] = stats.linregress(t_vals, mags)[0]\n            break\n        except:\n            continue\n    return features\n\ndef compute_transient_features_all(lc_df, log_df, desc=\"\"):\n    z_lookup = log_df.set_index('object_id')['Z'].to_dict()\n    results = []\n    object_ids = log_df['object_id'].tolist()\n    for i, oid in enumerate(object_ids):\n        if i % 500 == 0 and i > 0:\n            print(f\"  {desc}: {i}/{len(object_ids)}\")\n        obj_lc = lc_df[lc_df['object_id'] == oid]\n        feats = extract_transient_features(obj_lc, z_lookup.get(oid, 0))\n        feats['object_id'] = oid\n        results.append(feats)\n    return pd.DataFrame(results)\n","metadata":{"execution":{"iopub.status.busy":"2026-01-31T10:59:17.913089Z","iopub.execute_input":"2026-01-31T10:59:17.913432Z","iopub.status.idle":"2026-01-31T10:59:18.015478Z","shell.execute_reply.started":"2026-01-31T10:59:17.913405Z","shell.execute_reply":"2026-01-31T10:59:18.014613Z"},"papermill":{"duration":1834.89516,"end_time":"2026-01-22T17:04:32.861761","exception":false,"start_time":"2026-01-22T16:33:57.966601","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\ndef _extract_single_object(oid, g, z):\n    \"\"\"Extract features for a single object.\"\"\"\n    feat = {\"object_id\": oid}\n\n    flux = g[\"Flux_corr\"].values.astype(float)\n    flux_err = g[\"Fluxerr_corr\"].values.astype(float)\n    time = g[\"Time (MJD)\"].values.astype(float)\n    filt = g[\"Filter\"].values\n    is_detected = g[\"is_detected\"].values.astype(bool)\n    deviation = g[\"deviation\"].values.astype(float)\n    n = len(g)\n\n    feat[\"Z\"] = z\n    feat[\"n_obs\"] = int(n)\n    \n    if n < 2:\n        return feat\n\n    band_data = {}\n    for band in FILTERS:\n        mask = filt == band\n        if mask.sum() > 0:\n            band_data[band] = {\n                't': time[mask], 'f': flux[mask], 'f_err': flux_err[mask],\n                'det': is_detected[mask], 'n': int(mask.sum())\n            }\n        else:\n            band_data[band] = {'t': np.array([]), 'f': np.array([]), \n                               'f_err': np.array([]), 'det': np.array([], dtype=bool), 'n': 0}\n\n    n_detected = int(np.sum(is_detected))\n    feat[\"n_detected\"] = n_detected\n    feat[\"frac_detected\"] = float(n_detected / n) if n > 0 else 0.0\n    feat[\"max_deviation\"] = float(np.max(deviation))\n    feat[\"mean_deviation_detected\"] = float(np.mean(deviation[is_detected])) if n_detected > 0 else 0.0\n\n    def _cross_corr_fast(b1, b2, time_bins=15):\n        d1, d2 = band_data[b1], band_data[b2]\n        if d1['n'] < 3 or d2['n'] < 3:\n            return 0.0\n        t1, f1, t2, f2 = d1['t'], d1['f'], d2['t'], d2['f']\n        t_min, t_max = min(t1.min(), t2.min()), max(t1.max(), t2.max())\n        if t_max - t_min < 1:\n            return 0.0\n        bin_edges = np.linspace(t_min, t_max, time_bins + 1)\n        f1_binned = np.full(time_bins, np.nan)\n        f2_binned = np.full(time_bins, np.nan)\n        bins1 = np.clip(np.digitize(t1, bin_edges) - 1, 0, time_bins - 1)\n        bins2 = np.clip(np.digitize(t2, bin_edges) - 1, 0, time_bins - 1)\n        for i in range(time_bins):\n            m1, m2 = bins1 == i, bins2 == i\n            if m1.sum() > 0: f1_binned[i] = np.median(f1[m1])\n            if m2.sum() > 0: f2_binned[i] = np.median(f2[m2])\n        valid = np.isfinite(f1_binned) & np.isfinite(f2_binned)\n        if valid.sum() < 3:\n            return 0.0\n        x, y = f1_binned[valid], f2_binned[valid]\n        x, y = x - x.mean(), y - y.mean()\n        denom = np.sqrt(np.sum(x**2) * np.sum(y**2))\n        return float(np.sum(x * y) / denom) if denom > 1e-12 else 0.0\n\n    feat[\"cross_corr_gr\"] = _cross_corr_fast('g', 'r')\n    feat[\"cross_corr_gi\"] = _cross_corr_fast('g', 'i')\n    feat[\"cross_corr_ug\"] = _cross_corr_fast('u', 'g')\n    valid_corrs = [c for c in [feat[\"cross_corr_gr\"], feat[\"cross_corr_gi\"]] if c != 0]\n    feat[\"cross_corr_min\"] = float(np.min(valid_corrs)) if valid_corrs else 0.0\n\n    n_negative = int(np.sum(flux < 0))\n    n_positive = int(np.sum(flux > 0))\n    feat[\"n_negative\"] = n_negative\n    feat[\"n_positive\"] = n_positive\n    flux_err_safe = np.abs(flux_err) + 1e-12\n    feat[\"frac_significant_negative\"] = float(np.mean(flux < -2 * flux_err_safe))\n    feat[\"mean_negative_flux\"] = float(np.mean(flux[flux < 0])) if n_negative > 0 else 0.0\n\n    feat[\"longest_decline\"] = _longest_monotonic_run(flux[is_detected], 'decline') if n_detected >= 3 else 0\n\n    tmin, tmax = float(time.min()), float(time.max())\n    span = float(tmax - tmin)\n    feat[\"time_span\"] = span\n    dt = np.diff(time)\n    feat[\"dt_med\"] = float(np.median(dt)) if len(dt) else 0.0\n    feat[\"dt_max\"] = float(np.max(dt)) if len(dt) else 0.0\n    feat[\"flux_mad\"] = _safe_mad(flux)\n    feat[\"flux_kurtosis\"] = float(stats.kurtosis(flux)) if n > 3 else 0.0\n    snr = flux / (np.abs(flux_err) + 1e-12)\n    feat[\"snr_max\"] = float(np.max(snr))\n    feat[\"snr_min\"] = float(np.min(snr))\n\n    peak_idx = int(np.argmax(flux))\n    t_peak, f_peak = float(time[peak_idx]), float(flux[peak_idx])\n    feat[\"decay_time\"] = float(tmax - t_peak) if peak_idx < n-1 else 0.0\n    if f_peak > 0:\n        m_half = flux >= 0.5 * f_peak\n        feat[\"t_above_half\"] = float(time[m_half].max() - time[m_half].min()) if np.any(m_half) else 0.0\n    else:\n        feat[\"t_above_half\"] = 0.0\n    feat[\"von_neumann\"] = _von_neumann_ratio(flux)\n    pre, post = time < t_peak, time > t_peak\n    feat[\"slope_pre\"], _ = _lin_slope(time[pre], flux[pre])\n    feat[\"slope_post\"], feat[\"r2_post\"] = _lin_slope(time[post], flux[post])\n\n    alpha, alpha_r2 = _powerlaw_decay_index(time, flux, t_peak, is_detected)\n    feat[\"decay_alpha\"] = alpha\n    feat[\"decay_alpha_r2\"] = alpha_r2\n    feat[\"decay_alpha_deviation\"] = float(abs(alpha - 5/3))\n    feat[\"decay_is_tde_like\"] = float(1.0 / (feat[\"decay_alpha_deviation\"] + 0.1))\n    \n    def _decay_band(band):\n        bd = band_data[band]\n        if bd['n'] < 4: return 0.0, 0.0\n        m = (bd['t'] > t_peak) & (bd['f'] > 0) & bd['det']\n        if m.sum() < 4: return 0.0, 0.0\n        t_post, f_post = bd['t'][m] - t_peak, bd['f'][m]\n        m2 = t_post > 1.0\n        if m2.sum() < 4: return 0.0, 0.0\n        try:\n            slope, _, r, _, _ = stats.linregress(np.log(t_post[m2]), np.log(f_post[m2]))\n            return float(-slope), float(r**2)\n        except: return 0.0, 0.0\n\n    feat[\"decay_alpha_g\"], feat[\"decay_r2_g\"] = _decay_band(\"g\")\n    _, feat[\"decay_r2_r\"] = _decay_band(\"r\")\n    _, feat[\"decay_r2_i\"] = _decay_band(\"i\")\n    feat[\"decay_alpha_consistency\"] = float(1.0 / (abs(feat[\"decay_alpha_g\"] - alpha) + 0.1)) if feat[\"decay_alpha_g\"] > 0 and alpha > 0 else 0.0\n\n    # Peak prominence\n    flux_med = np.median(flux)\n    feat[\"peak_prominence\"] = float(np.clip(f_peak / (flux_med + 1e-10), 0, 100)) if flux_med > 0 else float(np.clip(f_peak - flux_med, 0, 1e6))\n\n    # Post-peak\n    if np.sum(post) >= 6:\n        f_post_arr, t_post_arr = flux[post], time[post]\n        feat[\"post_mean\"] = float(np.mean(f_post_arr))\n        mid_t = 0.5 * (t_post_arr.min() + t_post_arr.max())\n        late = t_post_arr > mid_t\n        feat[\"late_slope\"], _ = _lin_slope(t_post_arr[late], f_post_arr[late]) if np.sum(late) >= 3 else (0.0, 0.0)\n    else:\n        feat[\"post_mean\"] = feat[\"late_slope\"] = 0.0\n\n    for band in FILTERS:\n        bd = band_data[band]\n        feat[f\"n_{band}\"] = bd['n']\n        feat[f\"n_detected_{band}\"] = int(bd['det'].sum()) if bd['n'] > 0 else 0\n        if bd['n'] >= 1:\n            fb, tb = bd['f'], bd['t']\n            feat[f\"mean_{band}\"] = float(np.mean(fb))\n            feat[f\"std_{band}\"] = float(np.std(fb)) if bd['n'] > 1 else 0.0\n            feat[f\"p95_{band}\"] = float(np.percentile(fb, 95)) if bd['n'] > 1 else float(fb[0])\n            feat[f\"p05_{band}\"] = float(np.percentile(fb, 5)) if bd['n'] > 1 else float(fb[0])\n            feat[f\"amp_{band}\"] = float(feat[f\"p95_{band}\"] - feat[f\"p05_{band}\"])\n            ib = int(np.argmax(fb))\n            feat[f\"tpeak_{band}\"], feat[f\"fpeak_{band}\"] = float(tb[ib]), float(fb[ib])\n            feat[f\"frac_negative_{band}\"] = float(np.mean(fb < 0))\n            feat[f\"frac_detected_{band}\"] = float(np.mean(bd['det']))\n        else:\n            for k in [\"mean_\", \"std_\", \"p95_\", \"p05_\", \"amp_\", \"tpeak_\", \"fpeak_\"]:\n                feat[f\"{k}{band}\"] = 0.0\n            feat[f\"frac_negative_{band}\"] = feat[f\"frac_detected_{band}\"] = 0.0\n\n    feat[\"tlag_g_r\"] = float(feat[\"tpeak_g\"] - feat[\"tpeak_r\"]) if feat[\"tpeak_g\"] > 0 and feat[\"tpeak_r\"] > 0 else 0.0\n\n    def _color_fast(b1, b2):\n        d1, d2 = band_data[b1], band_data[b2]\n        if d1['n'] == 0 or d2['n'] == 0: return 0.0, 0.0, 0.0\n        t1, f1, det1 = d1['t'], d1['f'], d1['det']\n        t2, f2, det2 = d2['t'], d2['f'], d2['det']\n        idx = np.clip(np.searchsorted(t2, t1), 0, len(t2)-1)\n        idx_left = np.clip(idx - 1, 0, len(t2)-1)\n        idx_best = np.where(np.abs(t2[idx_left] - t1) < np.abs(t2[idx] - t1), idx_left, idx)\n        dt_best = np.abs(t2[idx_best] - t1)\n        m = (dt_best <= 1.0) & (f1 > 0) & (f2[idx_best] > 0) & det1 & det2[idx_best]\n        if np.sum(m) < 3: return 0.0, 0.0, 0.0\n        t_pair, col = t1[m], -2.5 * np.log10(f1[m] / f2[idx_best][m])\n        col_slope = stats.linregress(t_pair, col)[0] if len(col) >= 3 and (t_pair.max() - t_pair.min()) > 0 else 0.0\n        return float(np.median(col)), float(np.std(col)), float(col_slope)\n\n    _, feat[\"col_gr_std\"], col_gr_slope = _color_fast(\"g\", \"r\")\n    feat[\"col_ug_med\"], _, feat[\"col_ug_slope\"] = _color_fast(\"u\", \"g\")\n    feat[\"col_ri_med\"], _, _ = _color_fast(\"r\", \"i\")\n    feat[\"color_evolution_strength\"] = float(abs(col_gr_slope))\n\n    total_amp = feat[\"amp_u\"] + feat[\"amp_g\"] + feat[\"amp_r\"] + feat[\"amp_i\"] + feat[\"amp_z\"] + feat[\"amp_y\"] + 1e-10\n    feat[\"frac_blue\"] = float((feat[\"amp_u\"] + feat[\"amp_g\"]) / total_amp)\n    feat[\"frac_red\"] = float((feat[\"amp_i\"] + feat[\"amp_z\"] + feat[\"amp_y\"]) / total_amp)\n    feat[\"blue_red_amp_ratio\"] = float((feat[\"amp_u\"] + feat[\"amp_g\"]) / (feat[\"amp_i\"] + feat[\"amp_z\"] + feat[\"amp_y\"] + 1e-10))\n    feat[\"fpeak_g_over_r\"] = float(feat[\"fpeak_g\"] / (feat[\"fpeak_r\"] + 1e-10)) if feat[\"fpeak_r\"] > 0 else 0.0\n    feat[\"fpeak_u_over_r\"] = float(feat[\"fpeak_u\"] / (feat[\"fpeak_r\"] + 1e-10)) if feat[\"fpeak_r\"] > 0 else 0.0\n\n    feat[\"smooth_decay\"] = float((2 - feat[\"von_neumann\"]) * feat[\"decay_alpha\"])\n    denom = feat[\"fpeak_r\"] + feat[\"fpeak_i\"] + feat[\"fpeak_z\"]\n    feat[\"temp_proxy\"] = float((feat[\"fpeak_u\"] + feat[\"fpeak_g\"]) / (denom + 1e-10)) if denom > 0 else 0.0\n    feat[\"smooth_decay_proxy\"] = float(feat[\"r2_post\"] * feat[\"decay_alpha_r2\"])\n\n    if z > 0:\n        D_L = float(z * (1 + z/2))\n        feat[\"L_abs_proxy\"] = float(np.log10(max(f_peak * D_L**2 * (1 + z), 1e-10))) if f_peak > 0 else 0.0\n    else:\n        feat[\"L_abs_proxy\"] = 0.0\n\n    m = (time > t_peak) & (flux > 0) & is_detected\n    tt, ff = time[m] - t_peak, flux[m]\n    m2 = tt > 1e-3\n    exp_r2 = 0.0\n    if m2.sum() >= 4:\n        try:\n            _, _, r, _, _ = stats.linregress(tt[m2], np.log(ff[m2]))\n            exp_r2 = float(r**2)\n        except: pass\n    feat[\"powerlaw_vs_exp_r2\"] = float(alpha_r2 - exp_r2)\n    feat[\"powerlaw_exp_r2_ratio\"] = float((alpha_r2 + 0.01) / (exp_r2 + 0.01))\n    \n    return feat\n\ndef make_tde_features_fast(lc, log_df, n_jobs=4, use_parallel=True):\n    \"\"\"\n    Extract TDE features - with optional parallel processing.\n    \n    Args:\n        lc: Light curve DataFrame\n        log_df: Metadata DataFrame\n        n_jobs: Number of parallel workers (default 4)\n        use_parallel: Whether to use parallel processing\n    \"\"\"\n    import time as _time\n    start = _time.time()\n    \n    lc = lc.sort_values([\"object_id\", \"Time (MJD)\"])\n    z_lookup = log_df.set_index('object_id')['Z'].to_dict() if 'Z' in log_df.columns else {}\n    \n    # Pre-group data\n    grouped = list(lc.groupby(\"object_id\", sort=False))\n    n_total = len(grouped)\n    \n    print(f\"Extracting features for {n_total} objects...\")\n    \n    if use_parallel and n_total > 100:\n        # Parallel processing\n                results = Parallel(n_jobs=n_jobs, verbose=5)(\n            delayed(_extract_single_object)(oid, g, float(z_lookup.get(oid, 0.0)))\n            for oid, g in grouped\n        )\n    else:\n        # Sequential with progress\n        results = []\n        for i, (oid, g) in enumerate(grouped):\n            if i % 500 == 0:\n                print(f\"  {i}/{n_total} objects processed...\")\n            results.append(_extract_single_object(oid, g, float(z_lookup.get(oid, 0.0))))\n    \n    elapsed = _time.time() - start\n    print(f\"Feature extraction completed in {elapsed:.1f}s ({n_total/elapsed:.1f} obj/s)\")\n    \n    return pd.DataFrame(results)\n\ndef add_robust_coverage_features(df):\n    \"\"\"Add coverage features - only ones in TOP 100.\"\"\"\n    eps = 1e-9\n    df[\"obs_density\"] = df[\"n_obs\"] / (df[\"time_span\"] + 1.0)\n    \n    red_bands = ['i', 'z', 'y']\n    n_det_cols = [f\"n_detected_{b}\" for b in red_bands if f\"n_detected_{b}\" in df.columns]\n    df[\"red_detected\"] = df[n_det_cols].sum(axis=1) if n_det_cols else 0\n    \n    frac_cols = [f\"frac_detected_{b}\" for b in red_bands if f\"frac_detected_{b}\" in df.columns]\n    if frac_cols:\n        df[\"frac_red_detected\"] = df[frac_cols].mean(axis=1)\n    elif \"n_detected\" in df.columns:\n        df[\"frac_red_detected\"] = df[\"red_detected\"] / (df[\"n_detected\"] + eps)\n    else:\n        df[\"frac_red_detected\"] = 0.0\n    \n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T10:59:18.016551Z","iopub.execute_input":"2026-01-31T10:59:18.016833Z","iopub.status.idle":"2026-01-31T10:59:18.064525Z","shell.execute_reply.started":"2026-01-31T10:59:18.016804Z","shell.execute_reply":"2026-01-31T10:59:18.063595Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_feats = make_tde_features_fast(train_lc, train_log, n_jobs=4)\ntest_feats = make_tde_features_fast(test_lc, test_log, n_jobs=4)\n\ntrain_trans = compute_transient_features_all(train_lc, train_log, \"Train\")\ntest_trans = compute_transient_features_all(test_lc, test_log, \"Test\")\n\ntrans_cols = ['trans_delta_m15', 'trans_delta_m50', 'trans_fade_rate']\ntrain_feats = train_feats.merge(train_trans[['object_id'] + trans_cols], on='object_id', how='left')\ntest_feats = test_feats.merge(test_trans[['object_id'] + trans_cols], on='object_id', how='left')\n\ntrain_feats = add_robust_coverage_features(train_feats)\ntest_feats = add_robust_coverage_features(test_feats)\nprint(f\"Features: train {train_feats.shape}, test {test_feats.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T10:59:18.065601Z","iopub.execute_input":"2026-01-31T10:59:18.065905Z","iopub.status.idle":"2026-01-31T11:26:52.112927Z","shell.execute_reply.started":"2026-01-31T10:59:18.065862Z","shell.execute_reply":"2026-01-31T11:26:52.112143Z"}},"outputs":[{"name":"stdout","text":"Extracting features for 3043 objects...\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=4)]: Done 212 tasks      | elapsed:    8.9s\n[Parallel(n_jobs=4)]: Done 1652 tasks      | elapsed:   14.0s\n[Parallel(n_jobs=4)]: Done 3043 out of 3043 | elapsed:   19.1s finished\n","output_type":"stream"},{"name":"stdout","text":"Feature extraction completed in 19.6s (155.2 obj/s)\nExtracting features for 7135 objects...\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=4)]: Done 632 tasks      | elapsed:    2.5s\n[Parallel(n_jobs=4)]: Done 2072 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=4)]: Done 4088 tasks      | elapsed:   14.8s\n[Parallel(n_jobs=4)]: Done 6680 tasks      | elapsed:   24.1s\n[Parallel(n_jobs=4)]: Done 7135 out of 7135 | elapsed:   25.9s finished\n","output_type":"stream"},{"name":"stdout","text":"Feature extraction completed in 27.2s (261.9 obj/s)\n  Train: 500/3043\n  Train: 1000/3043\n  Train: 1500/3043\n  Train: 2000/3043\n  Train: 2500/3043\n  Train: 3000/3043\n  Test: 500/7135\n  Test: 1000/7135\n  Test: 1500/7135\n  Test: 2000/7135\n  Test: 2500/7135\n  Test: 3000/7135\n  Test: 3500/7135\n  Test: 4000/7135\n  Test: 4500/7135\n  Test: 5000/7135\n  Test: 5500/7135\n  Test: 6000/7135\n  Test: 6500/7135\n  Test: 7000/7135\nFeatures: train (3043, 130), test (7135, 130)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def compute_agn_features(lc, log_df):\n    \"\"\"\n    Compute AGN features - only features in TOP 100.\n    \"\"\"\n    lc = lc.sort_values([\"object_id\", \"Time (MJD)\"])\n    rows = []\n    \n    for oid, g in lc.groupby(\"object_id\"):\n        feat = {\"object_id\": oid}\n        \n        flux = g[\"Flux_corr\"].values\n        time = g[\"Time (MJD)\"].values\n        is_detected = g[\"is_detected\"].values if \"is_detected\" in g.columns else np.ones(len(flux), dtype=bool)\n        \n        n = len(flux)\n        if n < 5:\n            rows.append(feat)\n            continue\n        \n        # Find peak\n        peak_idx = np.argmax(flux)\n        t_peak = time[peak_idx]\n        f_peak = flux[peak_idx]\n        \n        pre_mask = time < t_peak\n        n_pre = pre_mask.sum()\n        \n        if n_pre >= 3:\n            flux_pre = flux[pre_mask]\n            feat[\"agn_pre_range\"] = float(np.max(flux_pre) - np.min(flux_pre))\n        else:\n            feat[\"agn_pre_range\"] = 0.0\n        \n        post_mask = time > t_peak\n        n_post = post_mask.sum()\n        \n        if n_post >= 5:\n            flux_post = flux[post_mask]\n            time_post = time[post_mask]\n            \n            # Count re-brightenings\n            n_rebrighten = 0\n            for i in range(1, len(flux_post)):\n                if flux_post[i] > flux_post[i-1] * 1.1:\n                    n_rebrighten += 1\n            feat[\"agn_n_rebrighten\"] = int(n_rebrighten)\n            feat[\"agn_rebrighten_rate\"] = float(n_rebrighten / (n_post - 1)) if n_post > 1 else 0.0\n            \n            # Monotonicity\n            if len(flux_post) >= 2:\n                declines = np.diff(flux_post) < 0\n                feat[\"agn_post_monotonicity\"] = float(np.mean(declines))\n            else:\n                feat[\"agn_post_monotonicity\"] = 0.0\n            \n            # Late-time behavior\n            late_mask = time_post > (t_peak + 0.7 * (time_post.max() - t_peak))\n            if late_mask.sum() >= 2:\n                flux_late = flux_post[late_mask]\n                feat[\"agn_late_std\"] = float(np.std(flux_late))\n                feat[\"agn_late_to_peak\"] = float(np.mean(flux_late) / (f_peak + 1e-10))\n            else:\n                feat[\"agn_late_std\"] = 0.0\n                feat[\"agn_late_to_peak\"] = 0.0\n        else:\n            feat[\"agn_n_rebrighten\"] = 0\n            feat[\"agn_rebrighten_rate\"] = 0.0\n            feat[\"agn_post_monotonicity\"] = 0.0\n            feat[\"agn_late_std\"] = 0.0\n            feat[\"agn_late_to_peak\"] = 0.0\n        \n        if n >= 5:\n            lags = [1, 5, 20]\n            for lag in lags:\n                if n > lag:\n                    sf = np.mean((flux[lag:] - flux[:-lag])**2)\n                    feat[f\"agn_sf_lag{lag}\"] = float(np.sqrt(sf))\n                else:\n                    feat[f\"agn_sf_lag{lag}\"] = 0.0\n            \n            if feat[\"agn_sf_lag1\"] > 0:\n                # Need lag10 for ratio\n                if n > 10:\n                    sf10 = np.mean((flux[10:] - flux[:-10])**2)\n                    feat[\"agn_sf_lag10\"] = float(np.sqrt(sf10))\n                else:\n                    feat[\"agn_sf_lag10\"] = 0.0\n                feat[\"agn_sf_ratio_10_1\"] = float(feat[\"agn_sf_lag10\"] / (feat[\"agn_sf_lag1\"] + 1e-10))\n            else:\n                feat[\"agn_sf_ratio_10_1\"] = 0.0\n        else:\n            for lag in [1, 5, 20]:\n                feat[f\"agn_sf_lag{lag}\"] = 0.0\n            feat[\"agn_sf_ratio_10_1\"] = 0.0\n        \n        rows.append(feat)\n    \n    return pd.DataFrame(rows)\n\n# Compute AGN features\nagn_train = compute_agn_features(train_lc, train_log)\nagn_test = compute_agn_features(test_lc, test_log)\n\n# Merge with existing features\ntrain_feats = train_feats.merge(agn_train, on='object_id', how='left')\ntest_feats = test_feats.merge(agn_test, on='object_id', how='left')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:26:52.114386Z","iopub.execute_input":"2026-01-31T11:26:52.114729Z","iopub.status.idle":"2026-01-31T11:26:56.143357Z","shell.execute_reply.started":"2026-01-31T11:26:52.114699Z","shell.execute_reply":"2026-01-31T11:26:56.142601Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if SNCOSMO_TRAIN_PATH.exists() and SNCOSMO_TEST_PATH.exists():\n    sncosmo_train = pd.read_parquet(SNCOSMO_TRAIN_PATH)\n    sncosmo_test = pd.read_parquet(SNCOSMO_TEST_PATH)\n    print(f\"Loaded train sncosmo: {sncosmo_train.shape}\")\n    print(f\"Loaded test sncosmo: {sncosmo_test.shape}\")\n    \n    # Select only TOP 100 features\n    sncosmo_key_cols = [\n        'object_id',\n        'sn_salt2_rchisq', 'sn_salt3_rchisq', \n        'sn_best_ia_rchisq', 'sn_is_good_ia_fit',  # Keep is_good_ia_fit for veto logic\n        'sn_salt2_x1',\n        'sn_best_cc_rchisq',\n    ]\n    sncosmo_key_cols = [c for c in sncosmo_key_cols if c in sncosmo_train.columns]\n    \n    # Merge with existing features\n    n_before = len(train_feats.columns)\n    train_feats = train_feats.merge(sncosmo_train[sncosmo_key_cols], on='object_id', how='left')\n    test_feats = test_feats.merge(sncosmo_test[sncosmo_key_cols], on='object_id', how='left')\n    print(f\"Added {len(train_feats.columns) - n_before} sncosmo features\")\nelse:\n    print(\"WARNING: SNCOSMO features not found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:26:56.146153Z","iopub.execute_input":"2026-01-31T11:26:56.146413Z","iopub.status.idle":"2026-01-31T11:26:56.308428Z","shell.execute_reply.started":"2026-01-31T11:26:56.146391Z","shell.execute_reply":"2026-01-31T11:26:56.307432Z"}},"outputs":[{"name":"stdout","text":"Loaded train sncosmo: (3043, 44)\nLoaded test sncosmo: (7135, 44)\nAdded 6 sncosmo features\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nimport time as _time\n\n# CONFIGURATION\nGP_CACHE_DIR = Path(\"/kaggle/input/gpfeatures/gp_cache/\")\nGP_CACHE_DIR.mkdir(exist_ok=True)\n\n# Cache file paths\nTRAIN_GP_CACHE = GP_CACHE_DIR / \"train_gp_features_v11.parquet\"\nTEST_GP_CACHE = GP_CACHE_DIR / \"test_gp_features_v11.parquet\"\n\n# Control flags\nUSE_GP_CACHE = True   # Set False to force recomputation\nSAVE_GP_CACHE = True  # Set False to skip saving (for testing)\n\n# GP hyperparameters\nGP_LENGTH_SCALE = 20  # days (observer frame)\nMIN_GP_POINTS = 5     # minimum points to fit GP\n\n# GP HELPER FUNCTIONS (only defined if not already present)\nif 'fit_gp_single_band_v11' not in dir():\n    from sklearn.gaussian_process import GaussianProcessRegressor\n    from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\n    \n    def flux_to_abmag_v11(flux_ujy):\n        \"\"\"Convert flux in μJy to AB magnitude.\"\"\"\n        flux_ujy = np.asarray(flux_ujy, dtype=float)\n        scalar_input = flux_ujy.ndim == 0\n        flux_ujy = np.atleast_1d(flux_ujy)\n        result = np.full_like(flux_ujy, np.nan, dtype=float)\n        valid = flux_ujy > 0\n        result[valid] = -2.5 * np.log10(flux_ujy[valid] * 1e-6) + 8.90\n        if scalar_input:\n            return float(result[0])\n        return result\n\n    def fit_gp_single_band_v11(t_obs, f, f_err):\n        \"\"\"Fit GP to a single band's light curve.\"\"\"\n        if len(t_obs) < MIN_GP_POINTS:\n            return None\n        kernel = (\n            ConstantKernel(1.0, (0.01, 100)) * \n            Matern(length_scale=GP_LENGTH_SCALE, length_scale_bounds=(5, 200), nu=2.5) +\n            WhiteKernel(noise_level=0.1, noise_level_bounds=(0.001, 10))\n        )\n        gp = GaussianProcessRegressor(\n            kernel=kernel, alpha=f_err**2 + 1e-6,\n            n_restarts_optimizer=2, normalize_y=True\n        )\n        try:\n            gp.fit(t_obs.reshape(-1, 1), f)\n            return gp\n        except:\n            return None\n\n    def compute_decay_slope_v11(t_rest, f, start_day, end_day):\n        \"\"\"Compute linear decay slope in rest-frame window.\"\"\"\n        mask = (t_rest >= start_day) & (t_rest <= end_day)\n        if mask.sum() < 3:\n            return np.nan, np.nan\n        t_fit, f_fit = t_rest[mask], f[mask]\n        if np.std(f_fit) < 1e-10:\n            return np.nan, np.nan\n        try:\n            slope, _, r_value, _, _ = stats.linregress(t_fit, f_fit)\n            return slope, r_value**2\n        except:\n            return np.nan, np.nan\n\n    def compute_time_above_v11(t_rest, f, f_peak, threshold_frac):\n        \"\"\"Time that flux stays above threshold (rest frame).\"\"\"\n        if f_peak <= 0:\n            return 0.0\n        above = f > threshold_frac * f_peak\n        if above.sum() == 0:\n            return 0.0\n        t_above = t_rest[above]\n        return t_above.max() - t_above.min()\n\n    print(\"  GP helper functions defined\")\n\n# MAIN GP FEATURE EXTRACTION FUNCTION\ndef extract_gp_features_single_v11(object_id, lc_df, z):\n    \"\"\"\n    Extract GP features for a single object.\n    All time-based features computed in REST FRAME.\n    \n    Returns: dict of features (without object_id)\n    \"\"\"\n    obj_lc = lc_df[lc_df['object_id'] == object_id]\n    \n    if len(obj_lc) < 10:\n        return {}\n    \n    # Rest-frame factor: t_rest = t_obs / (1 + z)\n    z_factor = 1 + z if z > 0 else 1.0\n    \n    features = {}\n    \n    # Create time grid for GP predictions\n    all_t = obj_lc['Time (MJD)'].values\n    t_min, t_max = all_t.min(), all_t.max()\n    t_grid = np.linspace(t_min, t_max, 500)\n    \n    # Fit GP to each band\n    band_data = {}\n    for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n        bd = obj_lc[obj_lc['Filter'] == band].sort_values('Time (MJD)')\n        if len(bd) < MIN_GP_POINTS:\n            continue\n        t = bd['Time (MJD)'].values\n        f = bd['Flux_corr'].values\n        f_err = np.abs(bd['Fluxerr_corr'].values) + 1e-6\n        \n        gp = fit_gp_single_band_v11(t, f, f_err)\n        if gp is None:\n            continue\n        \n        f_pred, _ = gp.predict(t_grid.reshape(-1, 1), return_std=True)\n        band_data[band] = {\n            'f_pred': f_pred,\n            't_peak_obs': t_grid[np.argmax(f_pred)],\n            'f_peak': f_pred.max()\n        }\n    \n    if len(band_data) == 0:\n        return features\n    \n    # Create pseudo-bolometric light curve\n    f_bol = np.zeros_like(t_grid)\n    for bd in band_data.values():\n        f_bol += np.clip(bd['f_pred'], 0, None)  # Only positive flux\n    \n    # Find bolometric peak\n    bol_peak_idx = np.argmax(f_bol)\n    t_peak_obs = t_grid[bol_peak_idx]\n    f_peak_bol = f_bol[bol_peak_idx]\n    \n    # Convert to REST FRAME relative to peak\n    # t_rest = 0 at peak, negative before, positive after\n    t_rest = (t_grid - t_peak_obs) / z_factor\n    \n    # ---------------------------------------------\n    # BOLOMETRIC FEATURES\n    # ---------------------------------------------\n    features['gp_f_peak_bol'] = f_peak_bol\n    \n    # Amplitude ratio (peak / baseline)\n    bol_baseline = np.percentile(f_bol, 10)\n    features['gp_amplitude_ratio'] = f_peak_bol / max(bol_baseline, 0.1)\n    \n    # Decay slopes at different windows (REST FRAME)\n    for window in [30, 60, 100]:\n        slope, r2 = compute_decay_slope_v11(t_rest, f_bol, 5, window)\n        features[f'gp_decay_slope_{window}d'] = slope\n        features[f'gp_decay_r2_{window}d'] = r2\n        \n        # Normalized by peak (dimensionless decay rate)\n        if not np.isnan(slope) and f_peak_bol > 0:\n            features[f'gp_decay_rate_norm_{window}d'] = slope / f_peak_bol\n        else:\n            features[f'gp_decay_rate_norm_{window}d'] = np.nan\n    \n    # Time above thresholds (REST FRAME)\n    features['gp_t_above_half'] = compute_time_above_v11(t_rest, f_bol, f_peak_bol, 0.5)\n    features['gp_t_above_10pct'] = compute_time_above_v11(t_rest, f_bol, f_peak_bol, 0.1)\n    \n    # ---------------------------------------------\n    # PER-BAND DECAY SLOPES\n    # ---------------------------------------------\n    for band in ['g', 'r', 'i', 'z']:\n        if band not in band_data:\n            features[f'gp_decay_slope_{band}'] = np.nan\n            features[f'gp_decay_rate_norm_{band}'] = np.nan\n            continue\n        \n        bd = band_data[band]\n        # Use same t_rest (relative to BOLOMETRIC peak)\n        slope, _ = compute_decay_slope_v11(t_rest, bd['f_pred'], 5, 60)\n        features[f'gp_decay_slope_{band}'] = slope\n        \n        if not np.isnan(slope) and bd['f_peak'] > 0:\n            features[f'gp_decay_rate_norm_{band}'] = slope / bd['f_peak']\n        else:\n            features[f'gp_decay_rate_norm_{band}'] = np.nan\n    \n    # ---------------------------------------------\n    # COLOR EVOLUTION (g-r)\n    # ---------------------------------------------\n    if 'g' in band_data and 'r' in band_data:\n        g_pred = band_data['g']['f_pred']\n        r_pred = band_data['r']['f_pred']\n        \n        # At peak (t_rest ~ 0)\n        idx_peak = np.argmin(np.abs(t_rest - 0))\n        g_peak, r_peak = g_pred[idx_peak], r_pred[idx_peak]\n        \n        # At 30d rest frame\n        idx_30d = np.argmin(np.abs(t_rest - 30))\n        g_30d, r_30d = g_pred[idx_30d], r_pred[idx_30d]\n        \n        if g_peak > 0 and r_peak > 0:\n            features['gp_color_gr_peak'] = flux_to_abmag_v11(g_peak) - flux_to_abmag_v11(r_peak)\n        else:\n            features['gp_color_gr_peak'] = np.nan\n        \n        if g_30d > 0 and r_30d > 0:\n            features['gp_color_gr_30d'] = flux_to_abmag_v11(g_30d) - flux_to_abmag_v11(r_30d)\n        else:\n            features['gp_color_gr_30d'] = np.nan\n        \n        # Color evolution: positive = getting redder, negative = getting bluer\n        if not np.isnan(features.get('gp_color_gr_peak', np.nan)) and \\\n           not np.isnan(features.get('gp_color_gr_30d', np.nan)):\n            features['gp_color_evolution_gr'] = features['gp_color_gr_30d'] - features['gp_color_gr_peak']\n        else:\n            features['gp_color_evolution_gr'] = np.nan\n    else:\n        features['gp_color_gr_peak'] = np.nan\n        features['gp_color_gr_30d'] = np.nan\n        features['gp_color_evolution_gr'] = np.nan\n    \n    return features\n\ndef compute_gp_features_all(lc_df, log_df, cache_path, desc=\"\"):\n    \"\"\"\n    Compute GP features for all objects with progress tracking and caching.\n    \n    Args:\n        lc_df: Light curve DataFrame (must have Flux_corr, Fluxerr_corr columns)\n        log_df: Metadata DataFrame (must have object_id, Z columns)\n        cache_path: Path to save/load parquet cache\n        desc: Description for progress messages\n    \n    Returns:\n        DataFrame with object_id and GP features\n    \"\"\"\n    # Check cache first\n    if USE_GP_CACHE and cache_path.exists():\n        print(f\"  Loading cached {desc} GP features from {cache_path}\")\n        df = pd.read_parquet(cache_path)\n        print(f\"  Loaded: {len(df)} objects, {len(df.columns)-1} features\")\n        return df\n    \n    # Compute features\n    print(f\"  Computing {desc} GP features (this takes ~10-15 min)...\")\n    z_lookup = log_df.set_index('object_id')['Z'].to_dict()\n    object_ids = log_df['object_id'].tolist()\n    \n    results = []\n    start_time = _time.time()\n    \n    for i, oid in enumerate(object_ids):\n        if i % 200 == 0:\n            elapsed = _time.time() - start_time\n            rate = (i + 1) / elapsed if elapsed > 0 else 0\n            eta = (len(object_ids) - i) / rate if rate > 0 else 0\n            print(f\"    {desc}: {i}/{len(object_ids)} ({rate:.1f}/s, ETA: {eta/60:.1f}min)\")\n        \n        z = z_lookup.get(oid, 0)\n        if pd.isna(z):\n            z = 0\n        \n        feats = extract_gp_features_single_v11(oid, lc_df, z)\n        feats['object_id'] = oid\n        results.append(feats)\n    \n    elapsed = _time.time() - start_time\n    print(f\"    {desc}: Done! {len(object_ids)} objects in {elapsed/60:.1f} min\")\n    \n    df = pd.DataFrame(results)\n    \n    # Reorder columns: object_id first\n    cols = ['object_id'] + [c for c in df.columns if c != 'object_id']\n    df = df[cols]\n    \n    # Save cache\n    if SAVE_GP_CACHE:\n        df.to_parquet(cache_path, index=False)\n        print(f\"    Saved to {cache_path}\")\n    \n    return df\n\n# COMPUTE OR LOAD GP FEATURES\nprint(\"\\nComputing/loading GP features...\")\n\ntrain_gp = compute_gp_features_all(train_lc, train_log, TRAIN_GP_CACHE, \"Train\")\ntest_gp = compute_gp_features_all(test_lc, test_log, TEST_GP_CACHE, \"Test\")\n\nprint(f\"\\nTrain GP: {train_gp.shape}, Test GP: {test_gp.shape}\")\n\n# DEFINE WHICH GP FEATURES TO USE (most significant from FP/FN analysis)\nGP_FEATURES_TO_USE = [\n    # Features in TOP 100 only\n    'gp_decay_slope_30d',\n    'gp_decay_slope_60d', \n    'gp_decay_slope_r',\n    'gp_decay_r2_100d',\n    'gp_f_peak_bol',\n    'gp_t_above_half',\n    'gp_color_evolution_gr',\n]\n\n# Filter to only features that exist\ngp_cols_available = [c for c in GP_FEATURES_TO_USE if c in train_gp.columns]\nprint(f\"\\nGP features to merge: {len(gp_cols_available)}\")\n\n# MERGE WITH EXISTING FEATURES\nprint(\"\\nMerging GP features with existing features...\")\n\n# Check for column name conflicts\nexisting_cols = set(train_feats.columns)\nnew_cols = set(gp_cols_available)\noverlap = existing_cols & new_cols\n\nif overlap:\n    print(f\"  WARNING: Dropping overlapping columns from existing: {overlap}\")\n    train_feats = train_feats.drop(columns=list(overlap), errors='ignore')\n    test_feats = test_feats.drop(columns=list(overlap), errors='ignore')\n\n# Merge\nmerge_cols = ['object_id'] + gp_cols_available\ntrain_feats = train_feats.merge(train_gp[merge_cols], on='object_id', how='left')\ntest_feats = test_feats.merge(test_gp[merge_cols], on='object_id', how='left')\n\nprint(f\"  Train features: {train_feats.shape}\")\nprint(f\"  Test features: {test_feats.shape}\")\n\n# Show GP feature coverage\nprint(\"\\nGP feature coverage (train):\")\nfor col in gp_cols_available[:6]:\n    cov = train_feats[col].notna().mean() * 100\n    mean_val = train_feats[col].mean()\n    print(f\"  {col}: {cov:.1f}% coverage, mean={mean_val:.4f}\")\nprint(\"  ...\")\n\n# Quick validation: compare TDE vs non-TDE\nif 'target' in train_log.columns:\n    print(\"\\nQuick validation (TDE vs non-TDE):\")\n    train_with_target = train_feats.merge(train_log[['object_id', 'target']], on='object_id')\n    tde = train_with_target[train_with_target['target'] == 1]\n    non_tde = train_with_target[train_with_target['target'] == 0]\n    \n    for col in ['gp_decay_slope_60d', 'gp_t_above_half', 'gp_color_evolution_gr']:\n        if col in train_feats.columns:\n            tde_mean = tde[col].mean()\n            non_mean = non_tde[col].mean()\n            print(f\"  {col}: TDE={tde_mean:.4f}, non-TDE={non_mean:.4f}\")\n\nprint(\"GP features integrated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:26:56.309881Z","iopub.execute_input":"2026-01-31T11:26:56.310335Z","iopub.status.idle":"2026-01-31T11:26:56.479845Z","shell.execute_reply.started":"2026-01-31T11:26:56.310307Z","shell.execute_reply":"2026-01-31T11:26:56.479124Z"}},"outputs":[{"name":"stdout","text":"  GP helper functions defined\n\nComputing/loading GP features...\n  Loading cached Train GP features from /kaggle/input/gpfeatures/gp_cache/train_gp_features_v11.parquet\n  Loaded: 3043 objects, 24 features\n  Loading cached Test GP features from /kaggle/input/gpfeatures/gp_cache/test_gp_features_v11.parquet\n  Loaded: 7135 objects, 24 features\n\nTrain GP: (3043, 25), Test GP: (7135, 25)\n\nGP features to merge: 7\n\nMerging GP features with existing features...\n  Train features: (3043, 154)\n  Test features: (7135, 154)\n\nGP feature coverage (train):\n  gp_decay_slope_30d: 98.2% coverage, mean=-0.1810\n  gp_decay_slope_60d: 98.2% coverage, mean=-0.1311\n  gp_decay_slope_r: 98.1% coverage, mean=-0.0408\n  gp_decay_r2_100d: 98.2% coverage, mean=0.8770\n  gp_f_peak_bol: 100.0% coverage, mean=17.0451\n  gp_t_above_half: 100.0% coverage, mean=361.4974\n  ...\n\nQuick validation (TDE vs non-TDE):\n  gp_decay_slope_60d: TDE=-0.1605, non-TDE=-0.1296\n  gp_t_above_half: TDE=146.4873, non-TDE=372.4931\n  gp_color_evolution_gr: TDE=-0.1528, non-TDE=-0.1604\nGP features integrated successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Validate features\n# Check which selected features are in train_feats\n# Note: gru_pred and transformer_pred come from sequence models later\nSELECTED_FEATURES = train_feats.columns\n# features_before_seq = [f for f in SELECTED_FEATURES if f not in ['gru_pred', 'transformer_pred']]\n\n# present = [f for f in features_before_seq if f in train_feats.columns]\n# missing = [f for f in features_before_seq if f not in train_feats.columns]\n\n# print(f\"\\nPresent: {len(present)}/{len(features_before_seq)} features\")\n# if missing:\n#     print(f\"\\nMISSING FEATURES ({len(missing)}):\")\n#     for m in missing:\n#         print(f\"  - {m}\")\n# else:\n#     print(\"\\nAll non-sequence features present!\")\n\nprint(f\"\\nTotal columns in train_feats: {len(train_feats.columns)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:49:18.589827Z","iopub.execute_input":"2026-01-31T11:49:18.590194Z","iopub.status.idle":"2026-01-31T11:49:18.595480Z","shell.execute_reply.started":"2026-01-31T11:49:18.590165Z","shell.execute_reply":"2026-01-31T11:49:18.594607Z"}},"outputs":[{"name":"stdout","text":"\nTotal columns in train_feats: 156\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## SECTION 7: SEQUENCE DATA PREPARATION","metadata":{"papermill":{"duration":0.005768,"end_time":"2026-01-22T17:04:32.873815","exception":false,"start_time":"2026-01-22T17:04:32.868047","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_enhanced_sequences(lc):\n    \"\"\"Build sequences with enhanced features including detection info.\"\"\"\n    lc = lc.sort_values([\"object_id\", \"Time (MJD)\"]).copy()\n    \n    seqs = {}\n    for oid, g in lc.groupby(\"object_id\"):\n        flux = g[\"Flux_corr\"].values.astype(\"float32\")\n        flux_err = g[\"Fluxerr_corr\"].values.astype(\"float32\")\n        time = g[\"Time (MJD)\"].values.astype(\"float32\")\n        fid = g[\"filter_id\"].values.astype(\"int64\")\n        is_detected = g[\"is_detected\"].values.astype(\"float32\")\n        deviation = g[\"deviation\"].values.astype(\"float32\")\n\n         # Get redshift for this object\n        z = g[\"Z\"].iloc[0]  # Same for all rows of this object\n        \n        # Convert observer-frame time to rest-frame time\n        dt_obs = np.diff(time, prepend=time[0]).astype(\"float32\")\n        dt_obs[0] = 0\n        dt = dt_obs# / (1 + z)\n\n        \n        flux_mean = flux.mean()\n        flux_std = flux.std() + 1e-6\n       # flux_norm = ((flux - flux_mean) / flux_std).astype(\"float32\")\n        snr = (flux / (np.abs(flux_err) + 1e-6)).astype(\"float32\")\n        log_err = np.log(np.abs(flux_err) + 1e-6).astype(\"float32\")\n        \n        flux_norm_raw = ((flux - flux_mean) / flux_std).astype(\"float32\")\n        snr_weight = np.clip(np.abs(snr) / 3.0, 0, 1).astype(\"float32\")\n        flux_norm = (flux_norm_raw * snr_weight).astype(\"float32\")\n        \n        flux_diff = np.diff(flux, prepend=flux[0]).astype(\"float32\")\n        flux_diff[0] = 0\n        \n\n        \n        seqs[oid] = {\n            \"dt\": dt, \"filter_id\": fid, \"flux\": flux, \"flux_err\": flux_err,\n            \"flux_norm\": flux_norm, \"flux_diff\": flux_diff, \"snr\": snr, \"log_err\": log_err,\n            \"is_detected\": is_detected, \"deviation\": deviation,\n        }\n    \n    return seqs\n\nprint(\"Building sequences...\")\ntrain_seqs = build_enhanced_sequences(train_lc)\ntest_seqs = build_enhanced_sequences(test_lc)\nprint(f\"Train sequences: {len(train_seqs)}, Test sequences: {len(test_seqs)}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:26:56.512104Z","iopub.execute_input":"2026-01-31T11:26:56.512415Z","iopub.status.idle":"2026-01-31T11:27:01.046930Z","shell.execute_reply.started":"2026-01-31T11:26:56.512389Z","shell.execute_reply":"2026-01-31T11:27:01.046270Z"},"papermill":{"duration":4.165052,"end_time":"2026-01-22T17:04:37.044648","exception":false,"start_time":"2026-01-22T17:04:32.879596","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Building sequences...\nTrain sequences: 3043, Test sequences: 7135\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## SECTION 8: GRU MODEL (v2 architecture with detection input)","metadata":{"papermill":{"duration":0.006156,"end_time":"2026-01-22T17:04:37.056945","exception":false,"start_time":"2026-01-22T17:04:37.050789","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class EnhancedSeqDataset(Dataset):\n    def __init__(self, ids, seqs, y=None,\n                 cad_drop_max=0.50, cad_drop_p=0.60, cad_drop_min=0.10,\n                 band_drop_p=0.30, band_drop_bias_blue=0.70, min_keep=10):\n        self.ids = ids\n        self.seqs = seqs\n        self.y = y\n        self.cad_drop_max = float(cad_drop_max)\n        self.cad_drop_p = float(cad_drop_p)\n        self.cad_drop_min = float(cad_drop_min)\n        self.band_drop_p = float(band_drop_p)\n        self.band_drop_bias_blue = float(band_drop_bias_blue)\n        self.min_keep = int(min_keep)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        oid = self.ids[idx]\n        s = self.seqs[oid]\n\n        # Add is_detected and deviation to input features\n        x_num = np.stack([\n            s[\"flux_norm\"], s[\"flux_diff\"], s[\"snr\"], s[\"log_err\"], s[\"dt\"],\n            s[\"is_detected\"], s[\"deviation\"]\n        ], axis=1).astype(np.float32)\n        x_fid = s[\"filter_id\"].astype(np.int64)\n        T = len(x_fid)\n\n        if self.y is not None and T > self.min_keep:\n            if np.random.rand() < self.band_drop_p:\n                if np.random.rand() < self.band_drop_bias_blue:\n                    drop_band = np.random.choice([f2i[\"u\"], f2i[\"g\"]])\n                else:\n                    drop_band = np.random.choice([f2i[f] for f in FILTERS])\n                m = (x_fid != drop_band)\n                if int(m.sum()) >= self.min_keep:\n                    x_num, x_fid = x_num[m], x_fid[m]\n                    T = len(x_fid)\n\n            if (self.cad_drop_max > 0) and (np.random.rand() < self.cad_drop_p) and (T > self.min_keep):\n                frac = np.random.uniform(self.cad_drop_min, self.cad_drop_max)\n                keep = max(self.min_keep, int(round((1.0 - frac) * T)))\n                if keep < T:\n                    idx_keep = np.sort(np.random.choice(T, size=keep, replace=False))\n                    x_num, x_fid = x_num[idx_keep], x_fid[idx_keep]\n\n        if self.y is None:\n            return oid, x_num, x_fid\n        return oid, x_num, x_fid, float(self.y[idx])\n\ndef collate_batch(batch):\n    has_y = (len(batch[0]) == 4)\n    oids = [b[0] for b in batch]\n    lens = [len(b[2]) for b in batch]\n    T = max(lens)\n    n_features = batch[0][1].shape[1]\n\n    x_num = torch.zeros(len(batch), T, n_features, dtype=torch.float32)\n    x_fid = torch.zeros(len(batch), T, dtype=torch.long)\n    mask = torch.zeros(len(batch), T, dtype=torch.float32)\n\n    for i, b in enumerate(batch):\n        L = len(b[2])\n        x_num[i, :L] = torch.from_numpy(b[1])\n        x_fid[i, :L] = torch.from_numpy(b[2])\n        mask[i, :L] = 1.0\n\n    if has_y:\n        y = torch.tensor([b[3] for b in batch], dtype=torch.float32)\n        return oids, x_num, x_fid, mask, y\n    return oids, x_num, x_fid, mask\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.Tanh(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n    \n    def forward(self, x, mask):\n        scores = self.attention(x).squeeze(-1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n        weights = F.softmax(scores, dim=1)\n        pooled = (x * weights.unsqueeze(-1)).sum(dim=1)\n        return pooled, weights\n\nclass AttentionGRUEncoder(nn.Module):\n    \"\"\"v2 architecture with detection input.\"\"\"\n    def __init__(self, n_filters=6, filt_emb=8, hidden=96, num_in=7, dropout=0.4, n_layers=2):\n        super().__init__()\n        self.femb = nn.Embedding(n_filters, filt_emb)\n        self.proj = nn.Sequential(\n            nn.Linear(num_in + filt_emb, hidden),\n            nn.LayerNorm(hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.gru = nn.GRU(hidden, hidden, num_layers=n_layers, batch_first=True,\n                         bidirectional=True, dropout=dropout if n_layers > 1 else 0)\n        self.ln = nn.LayerNorm(hidden * 2)\n        self.drop = nn.Dropout(dropout)\n        self.attention_pool = AttentionPooling(hidden * 2)\n        self.head = nn.Sequential(\n            nn.Linear(hidden * 2, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, 1)\n        )\n    \n    def forward(self, x_num, x_fid, mask):\n        e = self.femb(x_fid)\n        x = torch.cat([x_num, e], dim=-1)\n        x = self.proj(x)\n        x, _ = self.gru(x)\n        x = self.ln(x)\n        x = self.drop(x)\n        emb, _ = self.attention_pool(x, mask)\n        logit = self.head(emb).squeeze(-1)\n        return logit, emb","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:27:01.047962Z","iopub.execute_input":"2026-01-31T11:27:01.048315Z","iopub.status.idle":"2026-01-31T11:27:01.068541Z","shell.execute_reply.started":"2026-01-31T11:27:01.048289Z","shell.execute_reply":"2026-01-31T11:27:01.067663Z"},"papermill":{"duration":0.026464,"end_time":"2026-01-22T17:04:37.089339","exception":false,"start_time":"2026-01-22T17:04:37.062875","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## SECTION 9: TRANSFORMER MODEL (NEW in v9!)","metadata":{"papermill":{"duration":0.005976,"end_time":"2026-01-22T17:04:37.101423","exception":false,"start_time":"2026-01-22T17:04:37.095447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass TransformerEncoder(nn.Module):\n    \"\"\"Transformer-based encoder for light curves.\"\"\"\n    def __init__(self, n_filters=6, filt_emb=8, hidden=96, num_in=7, \n                 n_heads=4, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.hidden = hidden\n        \n        self.femb = nn.Embedding(n_filters, filt_emb)\n        self.input_proj = nn.Linear(num_in + filt_emb, hidden)\n        \n        self.pos_encoder = PositionalEncoding(hidden, dropout=dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden, nhead=n_heads, dim_feedforward=hidden*4,\n            dropout=dropout, activation='gelu', batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        self.attention_pool = AttentionPooling(hidden)\n        \n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, 1)\n        )\n    \n    def forward(self, x_num, x_fid, mask):\n        e = self.femb(x_fid)\n        x = torch.cat([x_num, e], dim=-1)\n        x = self.input_proj(x)\n        x = self.pos_encoder(x)\n        \n        # Transformer expects mask where True = ignore\n        src_key_padding_mask = (mask == 0)\n        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n        \n        emb, _ = self.attention_pool(x, mask)\n        logit = self.head(emb).squeeze(-1)\n        return logit, emb","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:27:01.069650Z","iopub.execute_input":"2026-01-31T11:27:01.070041Z","iopub.status.idle":"2026-01-31T11:27:01.088950Z","shell.execute_reply.started":"2026-01-31T11:27:01.070008Z","shell.execute_reply":"2026-01-31T11:27:01.088136Z"},"papermill":{"duration":0.01835,"end_time":"2026-01-22T17:04:37.125571","exception":false,"start_time":"2026-01-22T17:04:37.107221","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## SECTION 10: K-FOLD OOF TRAINING FOR SEQUENCE MODELS","metadata":{"papermill":{"duration":0.005748,"end_time":"2026-01-22T17:04:37.137375","exception":false,"start_time":"2026-01-22T17:04:37.131627","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def save_seq_cache(name, oof, test, aucs):\n    '''Save sequence model results to cache.'''\n    cache_path = CACHE_DIR / f\"{name.lower()}_cache.pt\"\n    torch.save({\n        'oof': oof,\n        'test': test,\n        'aucs': aucs,\n        'timestamp': pd.Timestamp.now().isoformat(),\n    }, cache_path)\n    print(f\"  Saved {name} cache to {cache_path}\")\n\ndef load_seq_cache(name, y):\n    '''Load sequence model results from cache.'''\n    cache_path = CACHE_DIR / f\"{name.lower()}_cache.pt\"\n    if not cache_path.exists():\n        print(f\"  Cache not found: {cache_path}\")\n        return None\n    cache = torch.load(cache_path, map_location='cpu',weights_only=False)\n    oof_auc = roc_auc_score(y, cache['oof'])\n    print(f\"  Loaded {name} cache (OOF AUC: {oof_auc:.4f}, from {cache.get('timestamp', 'unknown')})\")\n    return cache\n\ndef train_seq_model_fold(model_class, model_params, train_ids, train_seqs, y_train,\n                         val_ids, y_val, epochs=100, lr=1e-3, patience=30, device=DEVICE):\n    \"\"\"Generic training for GRU or Transformer.\"\"\"\n    ds_tr = EnhancedSeqDataset(train_ids, train_seqs, y=y_train,\n                               cad_drop_max=0.50, cad_drop_p=0.60, cad_drop_min=0.10,\n                               band_drop_p=0.30, band_drop_bias_blue=0.70, min_keep=10)\n    ds_va = EnhancedSeqDataset(val_ids, train_seqs, y=y_val,\n                               cad_drop_max=0.0, cad_drop_p=0.0, band_drop_p=0.0, min_keep=10)\n    \n    dl_tr = DataLoader(ds_tr, batch_size=128, shuffle=True, collate_fn=collate_batch, num_workers=0)\n    dl_va = DataLoader(ds_va, batch_size=256, shuffle=False, collate_fn=collate_batch, num_workers=0)\n    \n    model = model_class(**model_params).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.03)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-6)\n    \n    pos = y_train.sum()\n    neg = len(y_train) - pos\n    pos_weight = torch.tensor([neg / (pos + 1e-6)], device=device)\n    crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    \n    best_auc, best_state, no_improve = 0.0, None, 0\n    \n    for ep in range(1, epochs + 1):\n        model.train()\n        for batch in dl_tr:\n            oids, x_num, x_fid, mask, yb = batch\n            x_num, x_fid, mask, yb = x_num.to(device), x_fid.to(device), mask.to(device), yb.to(device)\n            opt.zero_grad()\n            logits, _ = model(x_num, x_fid, mask)\n            loss = crit(logits, yb)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n        scheduler.step()\n        \n        model.eval()\n        val_preds = []\n        with torch.no_grad():\n            for batch in dl_va:\n                oids, x_num, x_fid, mask, yb = batch\n                x_num, x_fid, mask = x_num.to(device), x_fid.to(device), mask.to(device)\n                logits, _ = model(x_num, x_fid, mask)\n                val_preds.append(torch.sigmoid(logits).cpu().numpy())\n        \n        val_preds = np.concatenate(val_preds)\n        auc = roc_auc_score(y_val, val_preds)\n        \n        if auc > best_auc:\n            best_auc = auc\n            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n            no_improve = 0\n        else:\n            no_improve += 1\n        \n        if no_improve >= patience:\n            break\n    \n    model.load_state_dict(best_state)\n    return model, best_auc\n\ndef predict_seq_model(model, ids, seqs, device=DEVICE):\n    \"\"\"Generic prediction for GRU or Transformer.\"\"\"\n    ds = EnhancedSeqDataset(ids, seqs, y=None, cad_drop_p=0.0, band_drop_p=0.0)\n    dl = DataLoader(ds, batch_size=256, shuffle=False, collate_fn=collate_batch, num_workers=0)\n    \n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in dl:\n            oids, x_num, x_fid, mask = batch\n            x_num, x_fid, mask = x_num.to(device), x_fid.to(device), mask.to(device)\n            logits, _ = model(x_num, x_fid, mask)\n            preds.append(torch.sigmoid(logits).cpu().numpy())\n    \n    return np.concatenate(preds)\n\ndef train_kfold_seq(model_name, model_class, model_params, train_ids, train_seqs, y, \n                    test_ids, test_seqs, n_folds=5, n_seeds=3):\n    '''K-fold training with caching support.'''\n    \n    # Try to load from cache first\n    if USE_CACHED_SEQ_MODELS:\n        cache = load_seq_cache(model_name, y)\n        if cache is not None:\n            return cache['oof'], cache['test']\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Training {model_name} ({n_folds} folds, {n_seeds} seeds)\")\n    print(f\"{'='*60}\")\n    \n    n_train = len(train_ids)\n    n_test = len(test_ids)\n    \n    oof_preds = np.zeros(n_train, dtype=np.float64)\n    test_preds = np.zeros(n_test, dtype=np.float64)\n    \n    fold_aucs = []\n    \n    for seed_idx in range(n_seeds):\n        seed = SEED + seed_idx * 77\n        set_seed(seed)\n        \n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n        \n        oof_seed = np.zeros(n_train, dtype=np.float64)\n        test_seed = np.zeros(n_test, dtype=np.float64)\n        \n        print(f\"\\nSeed {seed_idx+1}/{n_seeds} (seed={seed}):\")\n        \n        for fold, (tr_idx, va_idx) in enumerate(skf.split(train_ids, y), 1):\n            tr_ids = [train_ids[i] for i in tr_idx]\n            va_ids = [train_ids[i] for i in va_idx]\n            y_tr = y[tr_idx].astype(np.float32)\n            y_va = y[va_idx].astype(np.float32)\n            \n            model, auc = train_seq_model_fold(\n                model_class, model_params, tr_ids, train_seqs, y_tr,\n                va_ids, y_va, epochs=100, lr=1e-3, patience=30\n            )\n            \n            oof_seed[va_idx] = predict_seq_model(model, va_ids, train_seqs)\n            test_seed += predict_seq_model(model, test_ids, test_seqs) / n_folds\n            \n            fold_aucs.append(auc)\n            print(f\"  Fold {fold}: AUC={auc:.4f}\")\n        \n        oof_preds += oof_seed / n_seeds\n        test_preds += test_seed / n_seeds\n    \n    final_auc = roc_auc_score(y, oof_preds)\n    print(f\"\\n{model_name} Results:\")\n    print(f\"  Mean fold AUC: {np.mean(fold_aucs):.4f} ± {np.std(fold_aucs):.4f}\")\n    print(f\"  Final OOF AUC: {final_auc:.4f}\")\n    \n    # Save to cache\n    if SAVE_SEQ_MODELS:\n        save_seq_cache(model_name, oof_preds, test_preds, fold_aucs)\n    \n    return oof_preds, test_preds\n\n# Train sequence models\ntrain_ids_list = train_feats[\"object_id\"].tolist()\ntest_ids_list = test_feats[\"object_id\"].tolist()\ny = train_log.set_index(\"object_id\").loc[train_ids_list, \"target\"].values\n\n# GRU\ngru_params = dict(n_filters=6, filt_emb=8, hidden=96, num_in=7, dropout=0.4, n_layers=2)\noof_gru, test_gru = train_kfold_seq(\"GRU\", AttentionGRUEncoder, gru_params,\n                                     train_ids_list, train_seqs, y, test_ids_list, test_seqs,\n                                     n_folds=N_FOLDS, n_seeds=3)\n\n# Transformer\ntransformer_params = dict(n_filters=6, filt_emb=8, hidden=96, num_in=7, n_heads=4, n_layers=2, dropout=0.3)\noof_transformer, test_transformer = train_kfold_seq(\"Transformer\", TransformerEncoder, transformer_params,\n                                                      train_ids_list, train_seqs, y, test_ids_list, test_seqs,\n                                                      n_folds=N_FOLDS, n_seeds=3)\n\n# Compare\nbest_f1_gru = max(f1_score(y, (oof_gru >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nbest_f1_transformer = max(f1_score(y, (oof_transformer >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nprint(f\"\\nGRU OOF F1: {best_f1_gru:.4f}\")\nprint(f\"Transformer OOF F1: {best_f1_transformer:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:27:01.090148Z","iopub.execute_input":"2026-01-31T11:27:01.090592Z","iopub.status.idle":"2026-01-31T11:27:01.392920Z","shell.execute_reply.started":"2026-01-31T11:27:01.090556Z","shell.execute_reply":"2026-01-31T11:27:01.392071Z"},"papermill":{"duration":7975.294896,"end_time":"2026-01-22T19:17:32.438080","exception":false,"start_time":"2026-01-22T17:04:37.143184","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"  Loaded GRU cache (OOF AUC: 0.9544, from 2026-01-24T21:44:02.488192)\n  Loaded Transformer cache (OOF AUC: 0.8954, from 2026-01-24T23:20:26.479422)\n\nGRU OOF F1: 0.5596\nTransformer OOF F1: 0.4167\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class PlattScaler:\n    def __init__(self):\n        self.a = 1.0\n        self.b = 0.0\n    \n    def fit(self, probs, y_true):\n        from scipy.special import logit, expit\n        from scipy.optimize import minimize\n        probs = np.clip(probs, 1e-7, 1 - 1e-7)\n        logits = logit(probs)\n        def nll(params):\n            a, b = params\n            scaled = expit(a * logits + b)\n            scaled = np.clip(scaled, 1e-7, 1 - 1e-7)\n            return -np.mean(y_true * np.log(scaled) + (1 - y_true) * np.log(1 - scaled))\n        result = minimize(nll, [1.0, 0.0], method='L-BFGS-B')\n        self.a, self.b = result.x\n        return self\n    \n    def calibrate(self, probs):\n        from scipy.special import logit, expit\n        probs = np.clip(probs, 1e-7, 1 - 1e-7)\n        return expit(self.a * logit(probs) + self.b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:27:01.393935Z","iopub.execute_input":"2026-01-31T11:27:01.394210Z","iopub.status.idle":"2026-01-31T11:27:01.401222Z","shell.execute_reply.started":"2026-01-31T11:27:01.394181Z","shell.execute_reply":"2026-01-31T11:27:01.400421Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Add GRU and Transformer predictions to features\n\n# Optional: Apply Platt scaling for better calibration\nplatt_gru = PlattScaler().fit(oof_gru, y)\nplatt_trans = PlattScaler().fit(oof_transformer, y)\n\nprint(f\"GRU Platt: a={platt_gru.a:.3f}, b={platt_gru.b:.3f}\")\nprint(f\"Transformer Platt: a={platt_trans.a:.3f}, b={platt_trans.b:.3f}\")\n\n# Add to train features (OOF predictions)\ntrain_feats['gru_pred'] = platt_gru.calibrate(oof_gru)\ntrain_feats['transformer_pred'] = platt_trans.calibrate(oof_transformer)\n\n# Add to test features\ntest_feats['gru_pred'] = platt_gru.calibrate(test_gru)\ntest_feats['transformer_pred'] = platt_trans.calibrate(test_transformer)\n\nprint(f\"\\nTrain features shape: {train_feats.shape}\")\nprint(f\"Test features shape: {test_feats.shape}\")\n\n# Quick sanity check - verify AUC is preserved\nfrom sklearn.metrics import roc_auc_score\ngru_auc = roc_auc_score(y, train_feats['gru_pred'])\ntrans_auc = roc_auc_score(y, train_feats['transformer_pred'])\nprint(f\"\\nGRU AUC (after adding): {gru_auc:.4f}\")\nprint(f\"Transformer AUC (after adding): {trans_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:27:01.402391Z","iopub.execute_input":"2026-01-31T11:27:01.402655Z","iopub.status.idle":"2026-01-31T11:27:01.449225Z","shell.execute_reply.started":"2026-01-31T11:27:01.402629Z","shell.execute_reply":"2026-01-31T11:27:01.447694Z"}},"outputs":[{"name":"stdout","text":"GRU Platt: a=0.879, b=-2.461\nTransformer Platt: a=1.039, b=-2.582\n\nTrain features shape: (3043, 156)\nTest features shape: (7135, 156)\n\nGRU AUC (after adding): 0.9544\nTransformer AUC (after adding): 0.8954\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## SECTION 12: TREE MODELS","metadata":{"papermill":{"duration":0.007113,"end_time":"2026-01-22T19:17:34.972496","exception":false,"start_time":"2026-01-22T19:17:34.965383","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n# Prepare data\nfeature_cols = [c for c in train_feats.columns if c != 'object_id']\nX_train = train_feats[feature_cols].fillna(-999).values\nX_test = test_feats[feature_cols].fillna(-999).values\ny_train = train_log.set_index('object_id').loc[train_feats['object_id'], 'target'].values\n\n# Optional: StandardScaler (often not needed for tree models)\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)\n\npos = y_train.sum()\nneg = len(y_train) - pos\nscale_pos_weight = neg / pos\n\nprint(f\"Features: {len(feature_cols)}\")\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\nprint(f\"TDEs: {pos} ({100*pos/len(y_train):.1f}%), scale_pos_weight: {scale_pos_weight:.1f}\")\n\ndef train_lgb_cv(X, Xt, y, n_folds=5, n_seeds=3):\n    params = dict(\n        objective=\"binary\", learning_rate=0.03, n_estimators=5000,\n        num_leaves=31, max_depth=6, min_child_samples=10,\n        subsample=0.8, colsample_bytree=0.8,\n        reg_alpha=0.5, reg_lambda=1.0,\n        random_state=SEED,\n        n_jobs=-1, verbosity=-1\n    )\n    \n    oof = np.zeros(len(y), dtype=np.float64)\n    test_pred = np.zeros(len(Xt), dtype=np.float64)\n    \n    for seed_idx in range(n_seeds):\n        seed = SEED + seed_idx * 50\n        params[\"random_state\"] = seed\n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n        \n        for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n            model = lgb.LGBMClassifier(**params)\n            model.fit(X[tr_idx], y[tr_idx], eval_set=[(X[va_idx], y[va_idx])],\n                     callbacks=[lgb.early_stopping(300, verbose=False), lgb.log_evaluation(0)])\n            oof[va_idx] += model.predict_proba(X[va_idx])[:, 1] / n_seeds\n            test_pred += model.predict_proba(Xt)[:, 1] / (n_folds * n_seeds)\n    \n    return oof, test_pred\n\ndef train_xgb_cv(X, Xt, y, n_folds=5, n_seeds=3):\n    params = dict(\n        n_estimators=3000,\n        learning_rate=0.05,\n        max_depth=8,\n        min_child_weight=3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=SEED,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        early_stopping_rounds=100\n    )\n    \n    oof = np.zeros(len(y), dtype=np.float64)\n    test_pred = np.zeros(len(Xt), dtype=np.float64)\n    \n    for seed_idx in range(n_seeds):\n        seed = SEED + seed_idx * 50\n        params[\"random_state\"] = seed\n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n        \n        for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n            model = xgb.XGBClassifier(**params)\n            model.fit(X[tr_idx], y[tr_idx], eval_set=[(X[va_idx], y[va_idx])], verbose=False)\n            oof[va_idx] += model.predict_proba(X[va_idx])[:, 1] / n_seeds\n            test_pred += model.predict_proba(Xt)[:, 1] / (n_folds * n_seeds)\n    \n    return oof, test_pred\n\ndef train_catboost_cv(X, Xt, y, n_folds=5, n_seeds=3):\n    oof = np.zeros(len(y), dtype=np.float64)\n    test_pred = np.zeros(len(Xt), dtype=np.float64)\n    \n    for seed_idx in range(n_seeds):\n        seed = SEED + seed_idx * 50\n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n        \n        for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n            model = CatBoostClassifier(\n                iterations=5000, learning_rate=0.03, depth=6,\n                l2_leaf_reg=3.0, random_strength=0.5, bagging_temperature=0.5,\n                early_stopping_rounds=300,\n                verbose=False, random_seed=seed\n            )\n            model.fit(X[tr_idx], y[tr_idx], eval_set=(X[va_idx], y[va_idx]), use_best_model=True)\n            oof[va_idx] += model.predict_proba(X[va_idx])[:, 1] / n_seeds\n            test_pred += model.predict_proba(Xt)[:, 1] / (n_folds * n_seeds)\n    \n    return oof, test_pred\n\nprint(\"\\nTraining LightGBM...\")\noof_lgb, test_lgb = train_lgb_cv(X_train, X_test, y_train)\nf1_lgb = max(f1_score(y_train, (oof_lgb >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nprint(f\"  LGB OOF F1: {f1_lgb:.4f}\")\n\nprint(\"\\nTraining XGBoost...\")\noof_xgb, test_xgb = train_xgb_cv(X_train, X_test, y_train)\nf1_xgb = max(f1_score(y_train, (oof_xgb >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nprint(f\"  XGB OOF F1: {f1_xgb:.4f}\")\n\nprint(\"\\nTraining CatBoost...\")\noof_cat, test_cat = train_catboost_cv(X_train, X_test, y_train)\nf1_cat = max(f1_score(y_train, (oof_cat >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\nprint(f\"  CatBoost OOF F1: {f1_cat:.4f}\")\n\n# Grid search tree blend weights\nprint(\"\\nOptimizing tree blend weights...\")\nbest_tree_blend_f1 = 0\nbest_tree_weights = (1/3, 1/3, 1/3)\nbest_thr = 0.3\n\nfor xgb_w in np.arange(0.0, 1.01, 0.1):\n    for lgb_w in np.arange(0.0, 1.01 - xgb_w, 0.1):\n        cat_w = 1.0 - xgb_w - lgb_w\n        if cat_w < 0:\n            continue\n        blend = xgb_w * oof_xgb + lgb_w * oof_lgb + cat_w * oof_cat\n        for thr in np.linspace(0.1, 0.6, 51):\n            f1 = f1_score(y_train, (blend >= thr).astype(int))\n            if f1 > best_tree_blend_f1:\n                best_tree_blend_f1 = f1\n                best_tree_weights = (xgb_w, lgb_w, cat_w)\n                best_thr = thr\n\nprint(f\"Best tree weights: XGB={best_tree_weights[0]:.1f}, LGB={best_tree_weights[1]:.1f}, CAT={best_tree_weights[2]:.1f}\")\nprint(f\"Best threshold: {best_thr:.2f}\")\nprint(f\"Tree Blend OOF F1: {best_tree_blend_f1:.4f}\")\n\ntree_blend_oof = best_tree_weights[0] * oof_xgb + best_tree_weights[1] * oof_lgb + best_tree_weights[2] * oof_cat\ntree_blend_test = best_tree_weights[0] * test_xgb + best_tree_weights[1] * test_lgb + best_tree_weights[2] * test_cat\n\n# Final metrics\nfrom sklearn.metrics import precision_score, recall_score\noof_binary = (tree_blend_oof >= best_thr).astype(int)\nprint(f\"\\nFinal metrics @ threshold {best_thr:.2f}:\")\nprint(f\"  Precision: {precision_score(y_train, oof_binary):.4f}\")\nprint(f\"  Recall: {recall_score(y_train, oof_binary):.4f}\")\nprint(f\"  Predicted: {oof_binary.sum()} / {y_train.sum()} actual\")","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:27:37.503098Z","iopub.execute_input":"2026-01-31T11:27:37.503574Z","iopub.status.idle":"2026-01-31T11:30:42.331212Z","shell.execute_reply.started":"2026-01-31T11:27:37.503546Z","shell.execute_reply":"2026-01-31T11:30:42.330370Z"},"papermill":{"duration":206.587763,"end_time":"2026-01-22T19:21:01.567281","exception":false,"start_time":"2026-01-22T19:17:34.979518","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Features: 155\nTrain: 3043, Test: 7135\nTDEs: 148 (4.9%), scale_pos_weight: 19.6\n\nTraining LightGBM...\n  LGB OOF F1: 0.6599\n\nTraining XGBoost...\n  XGB OOF F1: 0.6726\n\nTraining CatBoost...\n  CatBoost OOF F1: 0.6372\n\nOptimizing tree blend weights...\nBest tree weights: XGB=0.4, LGB=0.4, CAT=0.2\nBest threshold: 0.24\nTree Blend OOF F1: 0.6730\n\nFinal metrics @ threshold 0.24:\n  Precision: 0.6347\n  Recall: 0.7162\n  Predicted: 167 / 148 actual\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## SECTION 13: BLEND WEIGHT OPTIMIZATION","metadata":{"papermill":{"duration":0.007063,"end_time":"2026-01-22T19:21:01.581424","exception":false,"start_time":"2026-01-22T19:21:01.574361","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define yy (same as y_train from previous cell)\nyy = y_train\ndef evaluate_ensemble(weights, oofs, tests, y):\n    \"\"\"Evaluate weighted ensemble.\"\"\"\n    blend_oof = sum(w * o for w, o in zip(weights, oofs))\n    blend_test = sum(w * t for w, t in zip(weights, tests))\n    \n    best_f1, best_thr = 0, 0.5\n    for t in np.linspace(0.1, 0.7, 61):\n        f1 = f1_score(y, (blend_oof >= t).astype(int))\n        if f1 > best_f1:\n            best_f1, best_thr = f1, t\n    \n    return best_f1, best_thr, blend_oof, blend_test\n\n# First: blend sequence models (GRU + Transformer)\nprint(\"\\nSequence model blend (GRU vs Transformer):\")\nbest_seq_f1 = 0\nbest_seq_blend = (0.5, 0.5)\nfor gru_w in [0.4, 0.5, 0.6, 0.7, 0.8]:\n    trans_w = 1 - gru_w\n    seq_oof = gru_w * oof_gru + trans_w * oof_transformer\n    f1 = max(f1_score(yy, (seq_oof >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\n    print(f\"  GRU={gru_w:.1f}, Trans={trans_w:.1f} -> F1={f1:.4f}\")\n    if f1 > best_seq_f1:\n        best_seq_f1 = f1\n        best_seq_blend = (gru_w, trans_w)\n\nprint(f\"Best seq blend: GRU={best_seq_blend[0]:.1f}, Trans={best_seq_blend[1]:.1f}\")\nseq_blend_oof = best_seq_blend[0] * oof_gru + best_seq_blend[1] * oof_transformer\nseq_blend_test = best_seq_blend[0] * test_gru + best_seq_blend[1] * test_transformer\n\n# Grid search: Tree vs Sequence blend\nprint(\"\\nGrid search (Tree vs Seq blend):\")\nprint(f\"{'Tree_W':<10} {'Seq_W':<10} {'OOF F1':<10} {'Threshold':<10}\")\nprint(\"-\" * 40)\n\nbest_config = None\nbest_f1_overall = 0\n\nfor tree_w in np.arange(0.35, 0.75, 0.05):\n    seq_w = 1 - tree_w\n    f1, thr, _, _ = evaluate_ensemble([tree_w, seq_w], [tree_blend_oof, seq_blend_oof],\n                                       [tree_blend_test, seq_blend_test], yy)\n    print(f\"{tree_w:<10.2f} {seq_w:<10.2f} {f1:<10.4f} {thr:<10.3f}\")\n    \n    if f1 > best_f1_overall:\n        best_f1_overall = f1\n        best_config = (tree_w, seq_w, thr)\n\nprint(f\"\\nBest: tree_w={best_config[0]:.2f}, seq_w={best_config[1]:.2f}, F1={best_f1_overall:.4f}\")\n\nbest_tree_w, best_seq_w, best_thr = best_config\nfinal_blend_oof = best_tree_w * tree_blend_oof + best_seq_w * seq_blend_oof\nfinal_blend_test = best_tree_w * tree_blend_test + best_seq_w * seq_blend_test","metadata":{"execution":{"iopub.status.busy":"2026-01-31T11:44:16.725944Z","iopub.execute_input":"2026-01-31T11:44:16.726917Z","iopub.status.idle":"2026-01-31T11:44:18.128763Z","shell.execute_reply.started":"2026-01-31T11:44:16.726881Z","shell.execute_reply":"2026-01-31T11:44:18.128010Z"},"papermill":{"duration":1.258844,"end_time":"2026-01-22T19:21:02.847388","exception":false,"start_time":"2026-01-22T19:21:01.588544","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\nSequence model blend (GRU vs Transformer):\n  GRU=0.4, Trans=0.6 -> F1=0.5562\n  GRU=0.5, Trans=0.5 -> F1=0.5799\n  GRU=0.6, Trans=0.4 -> F1=0.5739\n  GRU=0.7, Trans=0.3 -> F1=0.5882\n  GRU=0.8, Trans=0.2 -> F1=0.5772\nBest seq blend: GRU=0.7, Trans=0.3\n\nGrid search (Tree vs Seq blend):\nTree_W     Seq_W      OOF F1     Threshold \n----------------------------------------\n0.35       0.65       0.6397     0.610     \n0.40       0.60       0.6421     0.580     \n0.45       0.55       0.6438     0.520     \n0.50       0.50       0.6497     0.500     \n0.55       0.45       0.6538     0.480     \n0.60       0.40       0.6601     0.480     \n0.65       0.35       0.6623     0.460     \n0.70       0.30       0.6603     0.410     \n\nBest: tree_w=0.65, seq_w=0.35, F1=0.6623\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"submissions = {}\n\n# Best blend \npred_best = (final_blend_test >= best_thr).astype(int)\nsubmissions[\"best_blend\"] = {\n    \"pred\": pred_best,\n    \"desc\": f\"tree={best_tree_w:.2f}, seq={best_seq_w:.2f}, thr={best_thr:.3f}\",\n    \"oof_f1\": f1_score(yy, (final_blend_oof >= best_thr).astype(int)),\n}\n\n# Tree only\ntree_best_f1 = max(f1_score(yy, (tree_blend_oof >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\ntree_best_thr = [t for t in np.linspace(0.1, 0.7, 61) if f1_score(yy, (tree_blend_oof >= t).astype(int)) == tree_best_f1][0]\npred_tree = (tree_blend_test >= tree_best_thr).astype(int)\nsubmissions[\"tree_only\"] = {\n    \"pred\": pred_tree,\n    \"desc\": f\"Tree only, thr={tree_best_thr:.3f}\",\n    \"oof_f1\": tree_best_f1,\n}\n\n# GRU only\ngru_best_f1 = max(f1_score(yy, (oof_gru >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\ngru_best_thr = [t for t in np.linspace(0.1, 0.7, 61) if f1_score(yy, (oof_gru >= t).astype(int)) == gru_best_f1][0]\npred_gru = (test_gru >= gru_best_thr).astype(int)\nsubmissions[\"gru_only\"] = {\n    \"pred\": pred_gru,\n    \"desc\": \"GRU only\",\n    \"oof_f1\": gru_best_f1,\n}\n\n# Print summary\nprint(f\"\\\\n{'Name':<25} {'OOF F1':<10} {'Positives':<12} {'Rate':<10} {'Description'}\")\nprint(\"-\" * 85)\nfor name, data in submissions.items():\n    rate = data['pred'].sum() / len(data['pred'])\n    print(f\"{name:<25} {data['oof_f1']:<10.4f} {data['pred'].sum():<12} {rate:<10.4f} {data['desc']}\")\n\n# Save all submissions\nfor name, data in submissions.items():\n    sub_df = pd.DataFrame({\n        \"object_id\": test_feats[\"object_id\"].values,\n        \"target\": data[\"pred\"]\n    })\n    sub_df.to_csv(f\"submission_{name}.csv\", index=False)\n\nprint(f\"\\\\nSaved {len(submissions)} submission files!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:45:57.653184Z","iopub.execute_input":"2026-01-31T11:45:57.653849Z","iopub.status.idle":"2026-01-31T11:45:58.117623Z","shell.execute_reply.started":"2026-01-31T11:45:57.653816Z","shell.execute_reply":"2026-01-31T11:45:58.116948Z"}},"outputs":[{"name":"stdout","text":"\\nName                      OOF F1     Positives    Rate       Description\n-------------------------------------------------------------------------------------\nbest_blend                0.6623     331          0.0464     tree=0.65, seq=0.35, thr=0.460\ntree_only                 0.6730     372          0.0521     Tree only, thr=0.240\ngru_only                  0.5596     540          0.0757     GRU only\n\\nSaved 3 submission files!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## SECTION 14: GENERATE SUBMISSIONS","metadata":{"papermill":{"duration":0.007632,"end_time":"2026-01-22T19:21:02.863172","exception":false,"start_time":"2026-01-22T19:21:02.855540","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## SECTION 15: SAVE FP/FN FOR ANALYSIS","metadata":{"papermill":{"duration":0.007453,"end_time":"2026-01-22T19:21:03.545003","exception":false,"start_time":"2026-01-22T19:21:03.537550","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Save TP/FN for analysis\n# Get tree_only predictions at best threshold\ntree_best_f1 = max(f1_score(yy, (tree_blend_oof >= t).astype(int)) for t in np.linspace(0.1, 0.7, 61))\ntree_best_thr = [t for t in np.linspace(0.1, 0.7, 61) \n                 if f1_score(yy, (tree_blend_oof >= t).astype(int)) == tree_best_f1][0]\n\ntree_oof_pred = (tree_blend_oof >= tree_best_thr).astype(int)\n\n# Identify TP and FN\ntp_mask = (tree_oof_pred == 1) & (yy == 1)  # True Positives\nfn_mask = (tree_oof_pred == 0) & (yy == 1)  # False Negatives\nfp_mask = (tree_oof_pred == 1) & (yy == 0)  # False Positives\ntn_mask = (tree_oof_pred == 0) & (yy == 0)  # True Negatives\n\nprint(f\"\\nTree-only @ threshold={tree_best_thr:.3f}:\")\nprint(f\"  True Positives (TP):  {tp_mask.sum()}\")\nprint(f\"  False Negatives (FN): {fn_mask.sum()}\")\nprint(f\"  False Positives (FP): {fp_mask.sum()}\")\nprint(f\"  True Negatives (TN):  {tn_mask.sum()}\")\nprint(f\"  F1 Score: {tree_best_f1:.4f}\")\n\n# Create DataFrames with object details\ntp_objects = train_feats[tp_mask][['object_id']].copy()\ntp_objects['type'] = 'TP'\ntp_objects['prob'] = tree_blend_oof[tp_mask]\n\nfn_objects = train_feats[fn_mask][['object_id']].copy()\nfn_objects['type'] = 'FN'\nfn_objects['prob'] = tree_blend_oof[fn_mask]\n\nfp_objects = train_feats[fp_mask][['object_id']].copy()\nfp_objects['type'] = 'FP'\nfp_objects['prob'] = tree_blend_oof[fp_mask]\n\n# Combine and add useful features for analysis\ntree_only_analysis = pd.concat([tp_objects, fn_objects, fp_objects], ignore_index=True)\n\n# Add key features to help understand misclassifications\nkey_features = ['Z', 'n_detected', 'flux_max', 'decay_alpha', 'gru_pred', \n                'trans_delta_m15', 'sn_is_good_ia_fit']\nkey_features = [f for f in key_features if f in train_feats.columns]\n\nfor feat in key_features:\n    tree_only_analysis = tree_only_analysis.merge(\n        train_feats[['object_id', feat]], on='object_id', how='left'\n    )\n\n# Save to CSV\ntree_only_analysis.to_csv('tree_only_tp_fn_fp.csv', index=False)\nprint(f\"\\nSaved: tree_only_tp_fn_fp.csv\")\n\n# Also save just TP and FN for focused analysis\ntp_fn_only = tree_only_analysis[tree_only_analysis['type'].isin(['TP', 'FN'])]\ntp_fn_only.to_csv('tree_only_tp_fn.csv', index=False)\nprint(f\"Saved: tree_only_tp_fn.csv ({len(tp_fn_only)} objects)\")\n\n# Show FN analysis (these are TDEs we're missing)\nprint(\"\\n\" + \"-\"*50)\nprint(\"FALSE NEGATIVE ANALYSIS (TDEs we're missing):\")\nprint(\"-\"*50)\nfn_df = tree_only_analysis[tree_only_analysis['type'] == 'FN']\nprint(f\"Total FN: {len(fn_df)}\")\nprint(f\"\\nFN probability distribution:\")\nprint(f\"  Mean: {fn_df['prob'].mean():.4f}\")\nprint(f\"  Std:  {fn_df['prob'].std():.4f}\")\nprint(f\"  Min:  {fn_df['prob'].min():.4f}\")\nprint(f\"  Max:  {fn_df['prob'].max():.4f}\")\n\n# Show borderline FNs (close to threshold)\nborderline_fn = fn_df[fn_df['prob'] >= tree_best_thr - 0.1]\nprint(f\"\\nBorderline FN (prob >= {tree_best_thr-0.1:.2f}): {len(borderline_fn)}\")\nif len(borderline_fn) > 0:\n    print(borderline_fn[['object_id', 'prob'] + key_features[:3]].head(10).to_string())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T11:47:32.732576Z","iopub.execute_input":"2026-01-31T11:47:32.732929Z","iopub.status.idle":"2026-01-31T11:47:33.008393Z","shell.execute_reply.started":"2026-01-31T11:47:32.732885Z","shell.execute_reply":"2026-01-31T11:47:33.007493Z"}},"outputs":[{"name":"stdout","text":"\nTree-only @ threshold=0.240:\n  True Positives (TP):  106\n  False Negatives (FN): 42\n  False Positives (FP): 61\n  True Negatives (TN):  2834\n  F1 Score: 0.6730\n\nSaved: tree_only_tp_fn_fp.csv\nSaved: tree_only_tp_fn.csv (148 objects)\n\n--------------------------------------------------\nFALSE NEGATIVE ANALYSIS (TDEs we're missing):\n--------------------------------------------------\nTotal FN: 42\n\nFN probability distribution:\n  Mean: 0.0899\n  Std:  0.0653\n  Min:  0.0050\n  Max:  0.2196\n\nBorderline FN (prob >= 0.14): 11\n                 object_id      prob        Z  n_detected  decay_alpha\n112      amon_corch_idhren  0.162559  0.38890          51     0.238933\n114   anann_ungol_agarwaen  0.219566  0.08658          48     0.608426\n120   eilian_perian_glamor  0.206508  0.46410          53     0.458275\n124  gaurwaith_tamin_ungol  0.179621  0.31120          53     0.585562\n125   gwarth_bellas_glamor  0.168036  0.32490          40     1.056391\n129    hwiniol_randir_nell  0.181065  0.48610          54     0.268798\n137      neledh_gwend_naer  0.210589  0.61320          53     0.394381\n138     nell_tinuviel_adel  0.158379  0.21770          68     0.105068\n139      pennas_lunt_tinnu  0.162663  0.41950          45     0.273213\n141       ram_thavron_rusc  0.157862  0.15550          52     0.238621\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}